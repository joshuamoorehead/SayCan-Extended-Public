{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfYKd3ZPB6s_"
   },
   "source": [
    "### Copyright 2022 Google LLC. SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLzMRizDB2l5"
   },
   "source": [
    "Copyright 2022 Google LLC. SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GrV-nN-CTpl"
   },
   "source": [
    "# SayCan on a Robot Pick and Place Tabletop Environment\n",
    "\n",
    "[SayCan](https://say-can.github.io/) is an algorithm that grounds large language models with robotic affordances for long-horizon planning. Given a set of low-level robotic skills (e.g., \"put the green block in the red bowl\") and a high-level instruction (e.g., \"stack all the blocks\"), it scores what a language model believes will help forward the high-level instruction and scores what a robotic affordance model believes is possible. Together these give a task that is useful and possible and the robot executes the command.\n",
    "\n",
    "<img src=\"https://github.com/say-can/say-can.github.io/blob/main/img/saycan.png?raw=true\" height=\"320px\">\n",
    "\n",
    "This colab runs an example of SayCan for a pick and place robot on a table top.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/say-can/say-can.github.io/main/img/open_source_tabletop.png\" height=\"320px\">\n",
    "\n",
    "Models used: [GPT-3](https://arxiv.org/abs/2005.14165) (InstructGPT), [CLIP](https://arxiv.org/abs/2103.00020) (ViT-B/32), [ViLD](https://arxiv.org/abs/2104.13921), and [CLIPort](https://cliport.github.io/) variant ([Transporter Nets](https://transporternets.github.io/))\n",
    "\n",
    "### **Quick Start:**\n",
    "\n",
    "**Step 1.** Register for an [OpenAI API key](https://openai.com/blog/openai-api/) to use GPT-3 (there's a free trial) and enter it below\n",
    "\n",
    "**Step 2.** Menu > Change runtime type > Hardware accelerator > \"GPU\"\n",
    "\n",
    "**Step 3.** Menu > Runtime > Run all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2UdCCcpwbl0"
   },
   "source": [
    "### Setup and Installation\n",
    "Installs required packages (CLIP, PyTorch, etc.) and imports necessary libraries for vision, robotics, and machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2mFnHhPSTd_"
   },
   "outputs": [],
   "source": [
    "#@markdown Setup and Installation\n",
    "\n",
    "# Install required packages\n",
    "%pip install ftfy regex tqdm fvcore imageio==2.4.1 imageio-ffmpeg==0.4.5\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install -U --no-cache-dir gdown --pre\n",
    "%pip install pybullet moviepy flax==0.5.3 openai easydict imageio-ffmpeg\n",
    "%pip install tensorflow==2.7.0  # Uncomment if error: UNIMPLEMENTED: DNN library is not found.\n",
    "\n",
    "# Import required libraries\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Computer Vision and Image Processing\n",
    "import cv2\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import clip\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "# from flax.training import checkpoints\n",
    "# from flax.metrics import tensorboard\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "import tensorflow.compat.v1 as tf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Scientific Computing and Data Processing\n",
    "import numpy as np\n",
    "import optax\n",
    "from heapq import nlargest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utilities and Others\n",
    "from easydict import EasyDict\n",
    "import openai\n",
    "import pickle\n",
    "import pybullet\n",
    "\n",
    "import pybullet_data\n",
    "from tqdm import tqdm\n",
    "import IPython\n",
    "\n",
    "# Download required assets if they don't exist\n",
    "if not os.path.exists('ur5e/ur5e.urdf'):\n",
    "    !gdown --id 1Cc_fDSBL6QiDvNT4dpfAEbhbALSVoWcc\n",
    "    !gdown --id 1yOMEm-Zp_DL3nItG9RozPeJAmeOldekX\n",
    "    !gdown --id 1GsqNLhEl9dd4Mc3BM0dX3MibOI1FVWNM\n",
    "    !unzip ur5e.zip\n",
    "    !unzip robotiq_2f_85.zip\n",
    "    !unzip bowl.zip\n",
    "\n",
    "# Download ViLD pretrained model weights\n",
    "!gsutil cp -r gs://cloud-tpu-checkpoints/detection/projects/vild/colab/image_path_v2 ./\n",
    "\n",
    "# Setup TensorBoard\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "# Display GPU information\n",
    "!nvidia-smi\n",
    "\n",
    "# Check JAX backend\n",
    "from jax.lib import xla_bridge\n",
    "print(\"JAX backend:\", xla_bridge.get_backend().platform)\n",
    "# Configure OpenAI API\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in .env file\")\n",
    "openai.api_key = openai_api_key  # Set the API key for the openai package\n",
    "ENGINE = \"gpt-3.5-turbo-instruct\"\n",
    "# Note for scoring model, due to limitations of the GPT-3 api, each option \n",
    "# requires a separate call and can be expensive. Recommend iterating with ada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D6gTAdeX39yd"
   },
   "outputs": [],
   "source": [
    "#@markdown Global constants: pick and place objects, colors, workspace bounds\n",
    "\n",
    "PICK_TARGETS = {\n",
    "  \"blue block\": None,\n",
    "  \"red block\": None,\n",
    "  \"green block\": None,\n",
    "  \"yellow block\": None,\n",
    "}\n",
    "\n",
    "COLORS = {\n",
    "    \"blue\":   (78/255,  121/255, 167/255, 255/255),\n",
    "    \"red\":    (255/255,  87/255,  89/255, 255/255),\n",
    "    \"green\":  (89/255,  169/255,  79/255, 255/255),\n",
    "    \"yellow\": (237/255, 201/255,  72/255, 255/255),\n",
    "}\n",
    "\n",
    "PLACE_TARGETS = {\n",
    "  \"blue block\": None,\n",
    "  \"red block\": None,\n",
    "  \"green block\": None,\n",
    "  \"yellow block\": None,\n",
    "\n",
    "  \"blue bowl\": None,\n",
    "  \"red bowl\": None,\n",
    "  \"green bowl\": None,\n",
    "  \"yellow bowl\": None,\n",
    "\n",
    "  \"top left corner\":     (-0.3 + 0.05, -0.2 - 0.05, 0),\n",
    "  \"top right corner\":    (0.3 - 0.05,  -0.2 - 0.05, 0),\n",
    "  \"middle\":              (0,           -0.5,        0),\n",
    "  \"bottom left corner\":  (-0.3 + 0.05, -0.8 + 0.05, 0),\n",
    "  \"bottom right corner\": (0.3 - 0.05,  -0.8 + 0.05, 0),\n",
    "}\n",
    "\n",
    "PIXEL_SIZE = 0.00267857\n",
    "BOUNDS = np.float32([[-0.3, 0.3], [-0.8, -0.2], [0, 0.15]])  # X Y Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class + Initialization\n",
    "Main PyBullet environment implementation including:\n",
    "- Robot arm control\n",
    "- Object spawning and physics\n",
    "- Camera rendering\n",
    "- State management\n",
    "- Action execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ELFr8iE28hVX"
   },
   "outputs": [],
   "source": [
    "#@markdown Gripper (Robotiq 2F85) code\n",
    "\n",
    "class Robotiq2F85:\n",
    "  \"\"\"Gripper handling for Robotiq 2F85.\"\"\"\n",
    "\n",
    "  def __init__(self, robot, tool):\n",
    "    self.robot = robot\n",
    "    self.tool = tool\n",
    "    pos = [0.1339999999999999, -0.49199999999872496, 0.5]\n",
    "    rot = pybullet.getQuaternionFromEuler([np.pi, 0, np.pi])\n",
    "    urdf = \"robotiq_2f_85/robotiq_2f_85.urdf\"\n",
    "    self.body = pybullet.loadURDF(urdf, pos, rot)\n",
    "    self.n_joints = pybullet.getNumJoints(self.body)\n",
    "    self.activated = False\n",
    "\n",
    "    # Connect gripper base to robot tool.\n",
    "    pybullet.createConstraint(self.robot, tool, self.body, 0, jointType=pybullet.JOINT_FIXED, jointAxis=[0, 0, 0], parentFramePosition=[0, 0, 0], childFramePosition=[0, 0, -0.07], childFrameOrientation=pybullet.getQuaternionFromEuler([0, 0, np.pi / 2]))\n",
    "\n",
    "    # Set friction coefficients for gripper fingers.\n",
    "    for i in range(pybullet.getNumJoints(self.body)):\n",
    "      pybullet.changeDynamics(self.body, i, lateralFriction=10.0, spinningFriction=1.0, rollingFriction=1.0, frictionAnchor=True)\n",
    "\n",
    "    # Start thread to handle additional gripper constraints.\n",
    "    self.motor_joint = 1\n",
    "    self.constraints_thread = threading.Thread(target=self.step)\n",
    "    self.constraints_thread.daemon = True\n",
    "    self.constraints_thread.start()\n",
    "\n",
    "  # Control joint positions by enforcing hard contraints on gripper behavior.\n",
    "  # Set one joint as the open/close motor joint (other joints should mimic).\n",
    "  def step(self):\n",
    "    while True:\n",
    "      try:\n",
    "        currj = [pybullet.getJointState(self.body, i)[0] for i in range(self.n_joints)]\n",
    "        indj = [6, 3, 8, 5, 10]\n",
    "        targj = [currj[1], -currj[1], -currj[1], currj[1], currj[1]]\n",
    "        pybullet.setJointMotorControlArray(self.body, indj, pybullet.POSITION_CONTROL, targj, positionGains=np.ones(5))\n",
    "      except:\n",
    "        return\n",
    "      time.sleep(0.001)\n",
    "\n",
    "  # Close gripper fingers.\n",
    "  def activate(self):\n",
    "    pybullet.setJointMotorControl2(self.body, self.motor_joint, pybullet.VELOCITY_CONTROL, targetVelocity=1, force=10)\n",
    "    self.activated = True\n",
    "\n",
    "  # Open gripper fingers.\n",
    "  def release(self):\n",
    "    pybullet.setJointMotorControl2(self.body, self.motor_joint, pybullet.VELOCITY_CONTROL, targetVelocity=-1, force=10)\n",
    "    self.activated = False\n",
    "\n",
    "  # If activated and object in gripper: check object contact.\n",
    "  # If activated and nothing in gripper: check gripper contact.\n",
    "  # If released: check proximity to surface (disabled).\n",
    "  def detect_contact(self):\n",
    "    obj, _, ray_frac = self.check_proximity()\n",
    "    if self.activated:\n",
    "      empty = self.grasp_width() < 0.01\n",
    "      cbody = self.body if empty else obj\n",
    "      if obj == self.body or obj == 0:\n",
    "        return False\n",
    "      return self.external_contact(cbody)\n",
    "  #   else:\n",
    "  #     return ray_frac < 0.14 or self.external_contact()\n",
    "\n",
    "  # Return if body is in contact with something other than gripper\n",
    "  def external_contact(self, body=None):\n",
    "    if body is None:\n",
    "      body = self.body\n",
    "    pts = pybullet.getContactPoints(bodyA=body)\n",
    "    pts = [pt for pt in pts if pt[2] != self.body]\n",
    "    return len(pts) > 0  # pylint: disable=g-explicit-length-test\n",
    "\n",
    "  def check_grasp(self):\n",
    "    while self.moving():\n",
    "      time.sleep(0.001)\n",
    "    success = self.grasp_width() > 0.01\n",
    "    return success\n",
    "\n",
    "  def grasp_width(self):\n",
    "    lpad = np.array(pybullet.getLinkState(self.body, 4)[0])\n",
    "    rpad = np.array(pybullet.getLinkState(self.body, 9)[0])\n",
    "    dist = np.linalg.norm(lpad - rpad) - 0.047813\n",
    "    return dist\n",
    "\n",
    "  def check_proximity(self):\n",
    "    ee_pos = np.array(pybullet.getLinkState(self.robot, self.tool)[0])\n",
    "    tool_pos = np.array(pybullet.getLinkState(self.body, 0)[0])\n",
    "    vec = (tool_pos - ee_pos) / np.linalg.norm((tool_pos - ee_pos))\n",
    "    ee_targ = ee_pos + vec\n",
    "    ray_data = pybullet.rayTest(ee_pos, ee_targ)[0]\n",
    "    obj, link, ray_frac = ray_data[0], ray_data[1], ray_data[2]\n",
    "    return obj, link, ray_frac\n",
    "\n",
    "#@markdown Gym-style environment code\n",
    "\n",
    "class PickPlaceEnv():\n",
    "\n",
    "  def __init__(self):\n",
    "    self.dt = 1/480\n",
    "    self.sim_step = 0\n",
    "\n",
    "    # Configure and start PyBullet.\n",
    "    # python3 -m pybullet_utils.runServer\n",
    "    # pybullet.connect(pybullet.SHARED_MEMORY)  # pybullet.GUI for local GUI.\n",
    "    pybullet.connect(pybullet.DIRECT)  # pybullet.GUI for local GUI.\n",
    "    pybullet.configureDebugVisualizer(pybullet.COV_ENABLE_GUI, 0)\n",
    "    pybullet.setPhysicsEngineParameter(enableFileCaching=0)\n",
    "    assets_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    pybullet.setAdditionalSearchPath(assets_path)\n",
    "    pybullet.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "    pybullet.setTimeStep(self.dt)\n",
    "\n",
    "    self.home_joints = (np.pi / 2, -np.pi / 2, np.pi / 2, -np.pi / 2, 3 * np.pi / 2, 0)  # Joint angles: (J0, J1, J2, J3, J4, J5).\n",
    "    self.home_ee_euler = (np.pi, 0, np.pi)  # (RX, RY, RZ) rotation in Euler angles.\n",
    "    self.ee_link_id = 9  # Link ID of UR5 end effector.\n",
    "    self.tip_link_id = 10  # Link ID of gripper finger tips.\n",
    "    self.gripper = None\n",
    "\n",
    "  def reset(self, config):\n",
    "    pybullet.resetSimulation(pybullet.RESET_USE_DEFORMABLE_WORLD)\n",
    "    pybullet.setGravity(0, 0, -9.8)\n",
    "    self.cache_video = []\n",
    "\n",
    "    # Temporarily disable rendering to load URDFs faster.\n",
    "    pybullet.configureDebugVisualizer(pybullet.COV_ENABLE_RENDERING, 0)\n",
    "\n",
    "    # Add robot.\n",
    "    pybullet.loadURDF(\"plane.urdf\", [0, 0, -0.001])\n",
    "    self.robot_id = pybullet.loadURDF(\"ur5e/ur5e.urdf\", [0, 0, 0], flags=pybullet.URDF_USE_MATERIAL_COLORS_FROM_MTL)\n",
    "    self.ghost_id = pybullet.loadURDF(\"ur5e/ur5e.urdf\", [0, 0, -10])  # For forward kinematics.\n",
    "    self.joint_ids = [pybullet.getJointInfo(self.robot_id, i) for i in range(pybullet.getNumJoints(self.robot_id))]\n",
    "    self.joint_ids = [j[0] for j in self.joint_ids if j[2] == pybullet.JOINT_REVOLUTE]\n",
    "\n",
    "    # Move robot to home configuration.\n",
    "    for i in range(len(self.joint_ids)):\n",
    "      pybullet.resetJointState(self.robot_id, self.joint_ids[i], self.home_joints[i])\n",
    "\n",
    "    # Add gripper.\n",
    "    if self.gripper is not None:\n",
    "      while self.gripper.constraints_thread.is_alive():\n",
    "        self.constraints_thread_active = False\n",
    "    self.gripper = Robotiq2F85(self.robot_id, self.ee_link_id)\n",
    "    self.gripper.release()\n",
    "\n",
    "    # Add workspace.\n",
    "    plane_shape = pybullet.createCollisionShape(pybullet.GEOM_BOX, halfExtents=[0.3, 0.3, 0.001])\n",
    "    plane_visual = pybullet.createVisualShape(pybullet.GEOM_BOX, halfExtents=[0.3, 0.3, 0.001])\n",
    "    plane_id = pybullet.createMultiBody(0, plane_shape, plane_visual, basePosition=[0, -0.5, 0])\n",
    "    pybullet.changeVisualShape(plane_id, -1, rgbaColor=[0.2, 0.2, 0.2, 1.0])\n",
    "\n",
    "    # Load objects according to config.\n",
    "    self.config = config\n",
    "    self.obj_name_to_id = {}\n",
    "    obj_names = list(self.config[\"pick\"]) + list(self.config[\"place\"])\n",
    "    obj_xyz = np.zeros((0, 3))\n",
    "    for obj_name in obj_names:\n",
    "      if (\"block\" in obj_name) or (\"bowl\" in obj_name):\n",
    "\n",
    "        # Get random position 15cm+ from other objects.\n",
    "        while True:\n",
    "          rand_x = np.random.uniform(BOUNDS[0, 0] + 0.1, BOUNDS[0, 1] - 0.1)\n",
    "          rand_y = np.random.uniform(BOUNDS[1, 0] + 0.1, BOUNDS[1, 1] - 0.1)\n",
    "          rand_xyz = np.float32([rand_x, rand_y, 0.03]).reshape(1, 3)\n",
    "          if len(obj_xyz) == 0:\n",
    "            obj_xyz = np.concatenate((obj_xyz, rand_xyz), axis=0)\n",
    "            break\n",
    "          else:\n",
    "            nn_dist = np.min(np.linalg.norm(obj_xyz - rand_xyz, axis=1)).squeeze()\n",
    "            if nn_dist > 0.15:\n",
    "              obj_xyz = np.concatenate((obj_xyz, rand_xyz), axis=0)\n",
    "              break\n",
    "        \n",
    "        object_color = COLORS[obj_name.split(\" \")[0]]\n",
    "        object_type = obj_name.split(\" \")[1]\n",
    "        object_position = rand_xyz.squeeze()\n",
    "        if object_type == \"block\":\n",
    "          object_shape = pybullet.createCollisionShape(pybullet.GEOM_BOX, halfExtents=[0.02, 0.02, 0.02])\n",
    "          object_visual = pybullet.createVisualShape(pybullet.GEOM_BOX, halfExtents=[0.02, 0.02, 0.02])\n",
    "          object_id = pybullet.createMultiBody(0.01, object_shape, object_visual, basePosition=object_position)\n",
    "        elif object_type == \"bowl\":\n",
    "          object_position[2] = 0\n",
    "          object_id = pybullet.loadURDF(\"bowl/bowl.urdf\", object_position, useFixedBase=1)\n",
    "        pybullet.changeVisualShape(object_id, -1, rgbaColor=object_color)\n",
    "        self.obj_name_to_id[obj_name] = object_id\n",
    "\n",
    "    # Re-enable rendering.\n",
    "    pybullet.configureDebugVisualizer(pybullet.COV_ENABLE_RENDERING, 1)\n",
    "\n",
    "    for _ in range(200):\n",
    "      pybullet.stepSimulation()\n",
    "    return self.get_observation()\n",
    "\n",
    "  def servoj(self, joints):\n",
    "    \"\"\"Move to target joint positions with position control.\"\"\"\n",
    "    pybullet.setJointMotorControlArray(\n",
    "      bodyIndex=self.robot_id,\n",
    "      jointIndices=self.joint_ids,\n",
    "      controlMode=pybullet.POSITION_CONTROL,\n",
    "      targetPositions=joints,\n",
    "      positionGains=[0.01]*6)\n",
    "  \n",
    "  def movep(self, position):\n",
    "    \"\"\"Move to target end effector position.\"\"\"\n",
    "    joints = pybullet.calculateInverseKinematics(\n",
    "        bodyUniqueId=self.robot_id,\n",
    "        endEffectorLinkIndex=self.tip_link_id,\n",
    "        targetPosition=position,\n",
    "        targetOrientation=pybullet.getQuaternionFromEuler(self.home_ee_euler),\n",
    "        maxNumIterations=100)\n",
    "    self.servoj(joints)\n",
    "\n",
    "  def step(self, action=None):\n",
    "    \"\"\"Do pick and place motion primitive.\"\"\"\n",
    "    pick_xyz, place_xyz = action[\"pick\"].copy(), action[\"place\"].copy()\n",
    "\n",
    "    # Set fixed primitive z-heights.\n",
    "    hover_xyz = pick_xyz.copy() + np.float32([0, 0, 0.2])\n",
    "    pick_xyz[2] = 0.03\n",
    "    place_xyz[2] = 0.15\n",
    "\n",
    "    # Move to object.\n",
    "    ee_xyz = np.float32(pybullet.getLinkState(self.robot_id, self.tip_link_id)[0])\n",
    "    while np.linalg.norm(hover_xyz - ee_xyz) > 0.01:\n",
    "      self.movep(hover_xyz)\n",
    "      self.step_sim_and_render()\n",
    "      ee_xyz = np.float32(pybullet.getLinkState(self.robot_id, self.tip_link_id)[0])\n",
    "    while np.linalg.norm(pick_xyz - ee_xyz) > 0.01:\n",
    "      self.movep(pick_xyz)\n",
    "      self.step_sim_and_render()\n",
    "      ee_xyz = np.float32(pybullet.getLinkState(self.robot_id, self.tip_link_id)[0])\n",
    "\n",
    "    # Pick up object.\n",
    "    self.gripper.activate()\n",
    "    for _ in range(240):\n",
    "      self.step_sim_and_render()\n",
    "    while np.linalg.norm(hover_xyz - ee_xyz) > 0.01:\n",
    "      self.movep(hover_xyz)\n",
    "      self.step_sim_and_render()\n",
    "      ee_xyz = np.float32(pybullet.getLinkState(self.robot_id, self.tip_link_id)[0])\n",
    "    \n",
    "    # Move to place location.\n",
    "    while np.linalg.norm(place_xyz - ee_xyz) > 0.01:\n",
    "      self.movep(place_xyz)\n",
    "      self.step_sim_and_render()\n",
    "      ee_xyz = np.float32(pybullet.getLinkState(self.robot_id, self.tip_link_id)[0])\n",
    "\n",
    "    # Place down object.\n",
    "    while (not self.gripper.detect_contact()) and (place_xyz[2] > 0.03):\n",
    "      place_xyz[2] -= 0.001\n",
    "      self.movep(place_xyz)\n",
    "      for _ in range(3):\n",
    "        self.step_sim_and_render()\n",
    "    self.gripper.release()\n",
    "    for _ in range(240):\n",
    "      self.step_sim_and_render()\n",
    "    place_xyz[2] = 0.2\n",
    "    ee_xyz = np.float32(pybullet.getLinkState(self.robot_id, self.tip_link_id)[0])\n",
    "    while np.linalg.norm(place_xyz - ee_xyz) > 0.01:\n",
    "      self.movep(place_xyz)\n",
    "      self.step_sim_and_render()\n",
    "      ee_xyz = np.float32(pybullet.getLinkState(self.robot_id, self.tip_link_id)[0])\n",
    "    place_xyz = np.float32([0, -0.5, 0.2])\n",
    "    while np.linalg.norm(place_xyz - ee_xyz) > 0.01:\n",
    "      self.movep(place_xyz)\n",
    "      self.step_sim_and_render()\n",
    "      ee_xyz = np.float32(pybullet.getLinkState(self.robot_id, self.tip_link_id)[0])\n",
    "\n",
    "    observation = self.get_observation()\n",
    "    reward = self.get_reward()\n",
    "    done = False\n",
    "    info = {}\n",
    "    return observation, reward, done, info\n",
    "\n",
    "  def set_alpha_transparency(self, alpha: float) -> None:\n",
    "    for id in range(20):\n",
    "      visual_shape_data = pybullet.getVisualShapeData(id)\n",
    "      for i in range(len(visual_shape_data)):\n",
    "        object_id, link_index, _, _, _, _, _, rgba_color = visual_shape_data[i]\n",
    "        rgba_color = list(rgba_color[0:3]) +  [alpha]\n",
    "        pybullet.changeVisualShape(\n",
    "            self.robot_id, linkIndex=i, rgbaColor=rgba_color)      \n",
    "        pybullet.changeVisualShape(\n",
    "            self.gripper.body, linkIndex=i, rgbaColor=rgba_color)\n",
    "\n",
    "  def step_sim_and_render(self):\n",
    "    pybullet.stepSimulation()\n",
    "    self.sim_step += 1\n",
    "\n",
    "    # Render current image at 8 FPS.\n",
    "    if self.sim_step % 60 == 0:\n",
    "      self.cache_video.append(self.get_camera_image())\n",
    "\n",
    "  def get_camera_image(self):\n",
    "    image_size = (240, 240)\n",
    "    intrinsics = (120., 0, 120., 0, 120., 120., 0, 0, 1)\n",
    "    color, _, _, _, _ = env.render_image(image_size, intrinsics)\n",
    "    return color\n",
    "\n",
    "  def get_camera_image_top(self, \n",
    "                           image_size=(240, 240), \n",
    "                           intrinsics=(2000., 0, 2000., 0, 2000., 2000., 0, 0, 1),\n",
    "                           position=(0, -0.5, 5),\n",
    "                           orientation=(0, np.pi, -np.pi / 2),\n",
    "                           zrange=(0.01, 1.),\n",
    "                           set_alpha=True):\n",
    "    set_alpha and self.set_alpha_transparency(0)\n",
    "    color, _, _, _, _ = env.render_image_top(image_size, \n",
    "                                             intrinsics,\n",
    "                                             position,\n",
    "                                             orientation,\n",
    "                                             zrange)\n",
    "    set_alpha and self.set_alpha_transparency(1)\n",
    "    return color\n",
    "\n",
    "  def get_reward(self):\n",
    "    return 0  # TODO: check did the robot follow text instructions?\n",
    "\n",
    "  def get_observation(self):\n",
    "    observation = {}\n",
    "\n",
    "    # Render current image.\n",
    "    color, depth, position, orientation, intrinsics = self.render_image()\n",
    "\n",
    "    # Get heightmaps and colormaps.\n",
    "    points = self.get_pointcloud(depth, intrinsics)\n",
    "    position = np.float32(position).reshape(3, 1)\n",
    "    rotation = pybullet.getMatrixFromQuaternion(orientation)\n",
    "    rotation = np.float32(rotation).reshape(3, 3)\n",
    "    transform = np.eye(4)\n",
    "    transform[:3, :] = np.hstack((rotation, position))\n",
    "    points = self.transform_pointcloud(points, transform)\n",
    "    heightmap, colormap, xyzmap = self.get_heightmap(points, color, BOUNDS, PIXEL_SIZE)\n",
    "\n",
    "    observation[\"image\"] = colormap\n",
    "    observation[\"xyzmap\"] = xyzmap\n",
    "    observation[\"pick\"] = list(self.config[\"pick\"])\n",
    "    observation[\"place\"] = list(self.config[\"place\"])\n",
    "    return observation\n",
    "\n",
    "  def render_image(self, image_size=(720, 720), intrinsics=(360., 0, 360., 0, 360., 360., 0, 0, 1)):\n",
    "\n",
    "    # Camera parameters.\n",
    "    position = (0, -0.85, 0.4)\n",
    "    orientation = (np.pi / 4 + np.pi / 48, np.pi, np.pi)\n",
    "    orientation = pybullet.getQuaternionFromEuler(orientation)\n",
    "    zrange = (0.01, 10.)\n",
    "    noise=True\n",
    "\n",
    "    # OpenGL camera settings.\n",
    "    lookdir = np.float32([0, 0, 1]).reshape(3, 1)\n",
    "    updir = np.float32([0, -1, 0]).reshape(3, 1)\n",
    "    rotation = pybullet.getMatrixFromQuaternion(orientation)\n",
    "    rotm = np.float32(rotation).reshape(3, 3)\n",
    "    lookdir = (rotm @ lookdir).reshape(-1)\n",
    "    updir = (rotm @ updir).reshape(-1)\n",
    "    lookat = position + lookdir\n",
    "    focal_len = intrinsics[0]\n",
    "    znear, zfar = (0.01, 10.)\n",
    "    viewm = pybullet.computeViewMatrix(position, lookat, updir)\n",
    "    fovh = (image_size[0] / 2) / focal_len\n",
    "    fovh = 180 * np.arctan(fovh) * 2 / np.pi\n",
    "\n",
    "    # Notes: 1) FOV is vertical FOV 2) aspect must be float\n",
    "    aspect_ratio = image_size[1] / image_size[0]\n",
    "    projm = pybullet.computeProjectionMatrixFOV(fovh, aspect_ratio, znear, zfar)\n",
    "\n",
    "    # Render with OpenGL camera settings.\n",
    "    _, _, color, depth, segm = pybullet.getCameraImage(\n",
    "        width=image_size[1],\n",
    "        height=image_size[0],\n",
    "        viewMatrix=viewm,\n",
    "        projectionMatrix=projm,\n",
    "        shadow=1,\n",
    "        flags=pybullet.ER_SEGMENTATION_MASK_OBJECT_AND_LINKINDEX,\n",
    "        renderer=pybullet.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "    # Get color image.\n",
    "    color_image_size = (image_size[0], image_size[1], 4)\n",
    "    color = np.array(color, dtype=np.uint8).reshape(color_image_size)\n",
    "    color = color[:, :, :3]  # remove alpha channel\n",
    "    if noise:\n",
    "      color = np.int32(color)\n",
    "      color += np.int32(np.random.normal(0, 3, color.shape))\n",
    "      color = np.uint8(np.clip(color, 0, 255))\n",
    "\n",
    "    # Get depth image.\n",
    "    depth_image_size = (image_size[0], image_size[1])\n",
    "    zbuffer = np.float32(depth).reshape(depth_image_size)\n",
    "    depth = (zfar + znear - (2 * zbuffer - 1) * (zfar - znear))\n",
    "    depth = (2 * znear * zfar) / depth\n",
    "    if noise:\n",
    "      depth += np.random.normal(0, 0.003, depth.shape)\n",
    "\n",
    "    intrinsics = np.float32(intrinsics).reshape(3, 3)\n",
    "    return color, depth, position, orientation, intrinsics\n",
    "\n",
    "  def render_image_top(self, \n",
    "                       image_size=(240, 240), \n",
    "                       intrinsics=(2000., 0, 2000., 0, 2000., 2000., 0, 0, 1),\n",
    "                       position=(0, -0.5, 5),\n",
    "                       orientation=(0, np.pi, -np.pi / 2),\n",
    "                       zrange=(0.01, 1.)):\n",
    "\n",
    "    # Camera parameters.\n",
    "    orientation = pybullet.getQuaternionFromEuler(orientation)\n",
    "    noise=True\n",
    "\n",
    "    # OpenGL camera settings.\n",
    "    lookdir = np.float32([0, 0, 1]).reshape(3, 1)\n",
    "    updir = np.float32([0, -1, 0]).reshape(3, 1)\n",
    "    rotation = pybullet.getMatrixFromQuaternion(orientation)\n",
    "    rotm = np.float32(rotation).reshape(3, 3)\n",
    "    lookdir = (rotm @ lookdir).reshape(-1)\n",
    "    updir = (rotm @ updir).reshape(-1)\n",
    "    lookat = position + lookdir\n",
    "    focal_len = intrinsics[0]\n",
    "    znear, zfar = (0.01, 10.)\n",
    "    viewm = pybullet.computeViewMatrix(position, lookat, updir)\n",
    "    fovh = (image_size[0] / 2) / focal_len\n",
    "    fovh = 180 * np.arctan(fovh) * 2 / np.pi\n",
    "\n",
    "    # Notes: 1) FOV is vertical FOV 2) aspect must be float\n",
    "    aspect_ratio = image_size[1] / image_size[0]\n",
    "    projm = pybullet.computeProjectionMatrixFOV(fovh, aspect_ratio, znear, zfar)\n",
    "\n",
    "    # Render with OpenGL camera settings.\n",
    "    _, _, color, depth, segm = pybullet.getCameraImage(\n",
    "        width=image_size[1],\n",
    "        height=image_size[0],\n",
    "        viewMatrix=viewm,\n",
    "        projectionMatrix=projm,\n",
    "        shadow=1,\n",
    "        flags=pybullet.ER_SEGMENTATION_MASK_OBJECT_AND_LINKINDEX,\n",
    "        renderer=pybullet.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "    # Get color image.\n",
    "    color_image_size = (image_size[0], image_size[1], 4)\n",
    "    color = np.array(color, dtype=np.uint8).reshape(color_image_size)\n",
    "    color = color[:, :, :3]  # remove alpha channel\n",
    "    if noise:\n",
    "      color = np.int32(color)\n",
    "      color += np.int32(np.random.normal(0, 3, color.shape))\n",
    "      color = np.uint8(np.clip(color, 0, 255))\n",
    "\n",
    "    # Get depth image.\n",
    "    depth_image_size = (image_size[0], image_size[1])\n",
    "    zbuffer = np.float32(depth).reshape(depth_image_size)\n",
    "    depth = (zfar + znear - (2 * zbuffer - 1) * (zfar - znear))\n",
    "    depth = (2 * znear * zfar) / depth\n",
    "    if noise:\n",
    "      depth += np.random.normal(0, 0.003, depth.shape)\n",
    "\n",
    "    intrinsics = np.float32(intrinsics).reshape(3, 3)\n",
    "    return color, depth, position, orientation, intrinsics\n",
    "\n",
    "  def get_pointcloud(self, depth, intrinsics):\n",
    "    \"\"\"Get 3D pointcloud from perspective depth image.\n",
    "    Args:\n",
    "      depth: HxW float array of perspective depth in meters.\n",
    "      intrinsics: 3x3 float array of camera intrinsics matrix.\n",
    "    Returns:\n",
    "      points: HxWx3 float array of 3D points in camera coordinates.\n",
    "    \"\"\"\n",
    "    height, width = depth.shape\n",
    "    xlin = np.linspace(0, width - 1, width)\n",
    "    ylin = np.linspace(0, height - 1, height)\n",
    "    px, py = np.meshgrid(xlin, ylin)\n",
    "    px = (px - intrinsics[0, 2]) * (depth / intrinsics[0, 0])\n",
    "    py = (py - intrinsics[1, 2]) * (depth / intrinsics[1, 1])\n",
    "    points = np.float32([px, py, depth]).transpose(1, 2, 0)\n",
    "    return points\n",
    "\n",
    "  def transform_pointcloud(self, points, transform):\n",
    "    \"\"\"Apply rigid transformation to 3D pointcloud.\n",
    "    Args:\n",
    "      points: HxWx3 float array of 3D points in camera coordinates.\n",
    "      transform: 4x4 float array representing a rigid transformation matrix.\n",
    "    Returns:\n",
    "      points: HxWx3 float array of transformed 3D points.\n",
    "    \"\"\"\n",
    "    padding = ((0, 0), (0, 0), (0, 1))\n",
    "    homogen_points = np.pad(points.copy(), padding,\n",
    "                            \"constant\", constant_values=1)\n",
    "    for i in range(3):\n",
    "      points[Ellipsis, i] = np.sum(transform[i, :] * homogen_points, axis=-1)\n",
    "    return points\n",
    "\n",
    "  def get_heightmap(self, points, colors, bounds, pixel_size):\n",
    "    \"\"\"Get top-down (z-axis) orthographic heightmap image from 3D pointcloud.\n",
    "    Args:\n",
    "      points: HxWx3 float array of 3D points in world coordinates.\n",
    "      colors: HxWx3 uint8 array of values in range 0-255 aligned with points.\n",
    "      bounds: 3x2 float array of values (rows: X,Y,Z; columns: min,max) defining\n",
    "        region in 3D space to generate heightmap in world coordinates.\n",
    "      pixel_size: float defining size of each pixel in meters.\n",
    "    Returns:\n",
    "      heightmap: HxW float array of height (from lower z-bound) in meters.\n",
    "      colormap: HxWx3 uint8 array of backprojected color aligned with heightmap.\n",
    "      xyzmap: HxWx3 float array of XYZ points in world coordinates.\n",
    "    \"\"\"\n",
    "    width = int(np.round((bounds[0, 1] - bounds[0, 0]) / pixel_size))\n",
    "    height = int(np.round((bounds[1, 1] - bounds[1, 0]) / pixel_size))\n",
    "    heightmap = np.zeros((height, width), dtype=np.float32)\n",
    "    colormap = np.zeros((height, width, colors.shape[-1]), dtype=np.uint8)\n",
    "    xyzmap = np.zeros((height, width, 3), dtype=np.float32)\n",
    "\n",
    "    # Filter out 3D points that are outside of the predefined bounds.\n",
    "    ix = (points[Ellipsis, 0] >= bounds[0, 0]) & (points[Ellipsis, 0] < bounds[0, 1])\n",
    "    iy = (points[Ellipsis, 1] >= bounds[1, 0]) & (points[Ellipsis, 1] < bounds[1, 1])\n",
    "    iz = (points[Ellipsis, 2] >= bounds[2, 0]) & (points[Ellipsis, 2] < bounds[2, 1])\n",
    "    valid = ix & iy & iz\n",
    "    points = points[valid]\n",
    "    colors = colors[valid]\n",
    "\n",
    "    # Sort 3D points by z-value, which works with array assignment to simulate\n",
    "    # z-buffering for rendering the heightmap image.\n",
    "    iz = np.argsort(points[:, -1])\n",
    "    points, colors = points[iz], colors[iz]\n",
    "    px = np.int32(np.floor((points[:, 0] - bounds[0, 0]) / pixel_size))\n",
    "    py = np.int32(np.floor((points[:, 1] - bounds[1, 0]) / pixel_size))\n",
    "    px = np.clip(px, 0, width - 1)\n",
    "    py = np.clip(py, 0, height - 1)\n",
    "    heightmap[py, px] = points[:, 2] - bounds[2, 0]\n",
    "    for c in range(colors.shape[-1]):\n",
    "      colormap[py, px, c] = colors[:, c]\n",
    "      xyzmap[py, px, c] = points[:, c]\n",
    "    colormap = colormap[::-1, :, :]  # Flip up-down.\n",
    "    xv, yv = np.meshgrid(np.linspace(BOUNDS[0, 0], BOUNDS[0, 1], height),\n",
    "                         np.linspace(BOUNDS[1, 0], BOUNDS[1, 1], width))\n",
    "    xyzmap[:, :, 0] = xv\n",
    "    xyzmap[:, :, 1] = yv\n",
    "    xyzmap = xyzmap[::-1, :, :]  # Flip up-down.\n",
    "    heightmap = heightmap[::-1, :]  # Flip up-down.\n",
    "    return heightmap, colormap, xyzmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqJTWp1qWRoq"
   },
   "outputs": [],
   "source": [
    "#@markdown Initialize environment \n",
    "\n",
    "if 'env' in locals():\n",
    "  # Safely exit gripper threading before re-initializing environment.\n",
    "  env.gripper.running = False\n",
    "  while env.gripper.constraints_thread.isAlive():\n",
    "    time.sleep(0.01)\n",
    "env = PickPlaceEnv()\n",
    "#@markdown Render images.\n",
    "\n",
    "# Define and reset environment.\n",
    "config = {'pick':  ['yellow block', 'green block', 'blue block'],\n",
    "          'place': ['yellow bowl', 'green bowl', 'blue bowl']}\n",
    "\n",
    "np.random.seed(42)\n",
    "obs = env.reset(config)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "img = env.get_camera_image()\n",
    "plt.title('Perspective side-view')\n",
    "plt.imshow(img)\n",
    "plt.subplot(1, 2, 2)\n",
    "img = env.get_camera_image_top()\n",
    "img = np.flipud(img.transpose(1, 0, 2))\n",
    "plt.title('Orthographic top-view')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# Note: orthographic cameras do not exist. But we can approximate them by\n",
    "# projecting a 3D point cloud from an RGB-D camera, then unprojecting that onto\n",
    "# an orthographic plane. Orthographic views are useful for spatial action maps.\n",
    "plt.title('Unprojected orthographic top-view')\n",
    "plt.imshow(obs['image'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Control Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectExecutor:\n",
    "    \"\"\"Handles direct execution of pick and place actions\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "    def get_object_position(self, obj_name):\n",
    "        \"\"\"Get the position of an object in the environment\"\"\"\n",
    "        if obj_name in self.env.obj_name_to_id:\n",
    "            obj_id = self.env.obj_name_to_id[obj_name]\n",
    "            obj_pose = pybullet.getBasePositionAndOrientation(obj_id)\n",
    "            return np.float32(obj_pose[0])\n",
    "        elif obj_name in PLACE_TARGETS:\n",
    "            return np.float32(PLACE_TARGETS[obj_name])\n",
    "        return None\n",
    "        \n",
    "    def execute_action(self, obs, instruction):\n",
    "        \"\"\"Execute action without showing initial/final states\"\"\"\n",
    "        # Parse pick and place targets from instruction\n",
    "        split_text = instruction.lower().split(\"and\")\n",
    "        if len(split_text) != 2:\n",
    "            print(\"Invalid instruction format\")\n",
    "            return obs\n",
    "        \n",
    "        pick_text, place_text = split_text\n",
    "        \n",
    "        # Find pick target\n",
    "        pick_target = None\n",
    "        for name in PICK_TARGETS.keys():\n",
    "            if name in pick_text:\n",
    "                pick_target = name\n",
    "                break\n",
    "        \n",
    "        # Find place target    \n",
    "        place_target = None\n",
    "        for name in PLACE_TARGETS.keys():\n",
    "            if name in place_text:\n",
    "                place_target = name\n",
    "                break\n",
    "                \n",
    "        if not pick_target or not place_target:\n",
    "            print(\"Could not identify pick or place targets\")\n",
    "            return obs\n",
    "            \n",
    "        # Get object positions\n",
    "        pick_pos = self.get_object_position(pick_target)\n",
    "        place_pos = self.get_object_position(place_target)\n",
    "        \n",
    "        if pick_pos is None or place_pos is None:\n",
    "            print(\"Could not locate objects\")\n",
    "            return obs\n",
    "            \n",
    "        # Add some random noise to positions for robustness\n",
    "        pick_pos[:2] += np.random.normal(scale=0.01)\n",
    "        place_pos[:2] += np.random.normal(scale=0.01)\n",
    "        \n",
    "        # Execute action\n",
    "        try:\n",
    "            action = {'pick': pick_pos, 'place': place_pos}\n",
    "            new_obs, reward, done, info = self.env.step(action)\n",
    "            return new_obs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Execution failed: {e}\")\n",
    "            return obs\n",
    "\n",
    "    def run(self, obs, text):\n",
    "        \"\"\"Main function to run direct control execution\"\"\"\n",
    "        before = self.env.get_camera_image()\n",
    "        print(\"\\nInitial state:\")\n",
    "        plt.imshow(before)\n",
    "        plt.show()\n",
    "        \n",
    "        new_obs = self.execute_action(obs, text)\n",
    "        \n",
    "        print(\"\\nFinal state:\")\n",
    "        plt.imshow(self.env.get_camera_image())\n",
    "        plt.show()\n",
    "        \n",
    "        return new_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uyc6-l8IgQPS"
   },
   "source": [
    "## **Demo:** ViLD\n",
    "Run zero-shot open-vocabulary object detection with [ViLD](https://arxiv.org/abs/2104.13921) to generate a list of objects as a scene description for a large language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViLD Setup\n",
    "Configures Vision-Language Detection (ViLD) model for:\n",
    "- Object detection\n",
    "- Scene understanding\n",
    "- Visual grounding of language commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBkMaGRng1Vd"
   },
   "outputs": [],
   "source": [
    "# Define and reset environment.\n",
    "config = {'pick':  ['yellow block', 'green block', 'blue block'],\n",
    "          'place': ['yellow bowl', 'green bowl', 'blue bowl']}\n",
    "\n",
    "np.random.seed(42)\n",
    "obs = env.reset(config)\n",
    "img = env.get_camera_image_top()\n",
    "img = np.flipud(img.transpose(1, 0, 2))\n",
    "plt.title('ViLD Input Image')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "imageio.imwrite('tmp.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Model Configuration\n",
    "Sets up CLIP model for:\n",
    "- Text-to-image understanding\n",
    "- Visual feature extraction\n",
    "- Language understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Load CLIP model with memory management\n",
    "\n",
    "# Clear CUDA cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define device globally\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "\n",
    "def load_clip_model(max_retries=3, initial_batch_size=32):\n",
    "    \"\"\"Load CLIP model with automatic memory management\"\"\"\n",
    "    current_device = DEVICE  # Use global device setting\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Clear cache before loading\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Load model\n",
    "            clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=current_device)\n",
    "            clip_model.eval()\n",
    "            \n",
    "            # Print model info\n",
    "            print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n",
    "            print(\"Input resolution:\", clip_model.visual.input_resolution)\n",
    "            print(\"Context length:\", clip_model.context_length)\n",
    "            print(\"Vocab size:\", clip_model.vocab_size)\n",
    "            \n",
    "            return clip_model, clip_preprocess, initial_batch_size\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"CUDA out of memory on attempt {attempt + 1}. Error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                # Reduce batch size for next attempt\n",
    "                initial_batch_size = initial_batch_size // 2\n",
    "                print(f\"Reducing batch size to {initial_batch_size} and retrying...\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                time.sleep(1)  # Give GPU a moment to free memory\n",
    "            else:\n",
    "                print(\"Failed all attempts to load model. Trying CPU fallback...\")\n",
    "                current_device = \"cpu\"\n",
    "                clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=current_device)\n",
    "                clip_model.eval()\n",
    "                return clip_model, clip_preprocess, 16  # Smaller batch size for CPU\n",
    "\n",
    "# Load the model with automatic memory management\n",
    "try:\n",
    "    clip_model, clip_preprocess, batch_size = load_clip_model()\n",
    "    print(f\"Successfully loaded CLIP model on {DEVICE} with batch size {batch_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load CLIP model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "l-qWw2WDh_dp"
   },
   "outputs": [],
   "source": [
    "#@markdown Define ViLD hyperparameters.\n",
    "FLAGS = {\n",
    "    'prompt_engineering': True,\n",
    "    'this_is': True,\n",
    "    'temperature': 100.0,\n",
    "    'use_softmax': False,\n",
    "}\n",
    "FLAGS = EasyDict(FLAGS)\n",
    "\n",
    "\n",
    "# # Global matplotlib settings\n",
    "# SMALL_SIZE = 16#10\n",
    "# MEDIUM_SIZE = 18#12\n",
    "# BIGGER_SIZE = 20#14\n",
    "\n",
    "# plt.rc('font', size=MEDIUM_SIZE)         # controls default text sizes\n",
    "# plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "# plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "# plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('legend', fontsize=MEDIUM_SIZE)   # legend fontsize\n",
    "# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "# Parameters for drawing figure.\n",
    "display_input_size = (10, 10)\n",
    "overall_fig_size = (18, 24)\n",
    "\n",
    "line_thickness = 1\n",
    "fig_size_w = 35\n",
    "# fig_size_h = min(max(5, int(len(category_names) / 2.5) ), 10)\n",
    "mask_color =   'red'\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ViLD Prompt Engineering Cell\n",
    "- **Purpose**: Create language templates for object detection\n",
    "- **Key Functions**: \n",
    " - `article()`: Determines \"a\" vs \"an\"\n",
    " - `processed_name()`: Cleans object names\n",
    " - `build_text_embedding()`: Creates CLIP text embeddings\n",
    "- **Templates**: Multiple prompt formats like:\n",
    " - \"a photo of {}\"\n",
    " - \"This is a {}\"\n",
    " - Various quality descriptions (good/bad/bright/dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "TOtCs2B9iDMJ"
   },
   "outputs": [],
   "source": [
    "#@markdown ViLD prompt engineering.\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def article(name):\n",
    "  return \"an\" if name[0] in \"aeiou\" else \"a\"\n",
    "\n",
    "def processed_name(name, rm_dot=False):\n",
    "  # _ for lvis\n",
    "  # / for obj365\n",
    "  res = name.replace(\"_\", \" \").replace(\"/\", \" or \").lower()\n",
    "  if rm_dot:\n",
    "    res = res.rstrip(\".\")\n",
    "  return res\n",
    "\n",
    "single_template = [\n",
    "    \"a photo of {article} {}.\"\n",
    "]\n",
    "\n",
    "# multiple_templates = [\n",
    "#     \"There is {article} {} in the scene.\",\n",
    "#     \"a painting of a {}.\",\n",
    "# ]\n",
    "\n",
    "multiple_templates = [\n",
    "    'There is {article} {} in the scene.',\n",
    "    'There is the {} in the scene.',\n",
    "    'a photo of {article} {} in the scene.',\n",
    "    'a photo of the {} in the scene.',\n",
    "    'a photo of one {} in the scene.',\n",
    "\n",
    "\n",
    "    'itap of {article} {}.',\n",
    "    'itap of my {}.',  # itap: I took a picture of\n",
    "    'itap of the {}.',\n",
    "    'a photo of {article} {}.',\n",
    "    'a photo of my {}.',\n",
    "    'a photo of the {}.',\n",
    "    'a photo of one {}.',\n",
    "    'a photo of many {}.',\n",
    "\n",
    "    'a good photo of {article} {}.',\n",
    "    'a good photo of the {}.',\n",
    "    'a bad photo of {article} {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a photo of a nice {}.',\n",
    "    'a photo of the nice {}.',\n",
    "    'a photo of a cool {}.',\n",
    "    'a photo of the cool {}.',\n",
    "    'a photo of a weird {}.',\n",
    "    'a photo of the weird {}.',\n",
    "\n",
    "    'a photo of a small {}.',\n",
    "    'a photo of the small {}.',\n",
    "    'a photo of a large {}.',\n",
    "    'a photo of the large {}.',\n",
    "\n",
    "    'a photo of a clean {}.',\n",
    "    'a photo of the clean {}.',\n",
    "    'a photo of a dirty {}.',\n",
    "    'a photo of the dirty {}.',\n",
    "\n",
    "    'a bright photo of {article} {}.',\n",
    "    'a bright photo of the {}.',\n",
    "    'a dark photo of {article} {}.',\n",
    "    'a dark photo of the {}.',\n",
    "\n",
    "    'a photo of a hard to see {}.',\n",
    "    'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of {article} {}.',\n",
    "    'a low resolution photo of the {}.',\n",
    "    'a cropped photo of {article} {}.',\n",
    "    'a cropped photo of the {}.',\n",
    "    'a close-up photo of {article} {}.',\n",
    "    'a close-up photo of the {}.',\n",
    "    'a jpeg corrupted photo of {article} {}.',\n",
    "    'a jpeg corrupted photo of the {}.',\n",
    "    'a blurry photo of {article} {}.',\n",
    "    'a blurry photo of the {}.',\n",
    "    'a pixelated photo of {article} {}.',\n",
    "    'a pixelated photo of the {}.',\n",
    "\n",
    "    'a black and white photo of the {}.',\n",
    "    'a black and white photo of {article} {}.',\n",
    "\n",
    "    'a plastic {}.',\n",
    "    'the plastic {}.',\n",
    "\n",
    "    'a toy {}.',\n",
    "    'the toy {}.',\n",
    "    'a plushie {}.',\n",
    "    'the plushie {}.',\n",
    "    'a cartoon {}.',\n",
    "    'the cartoon {}.',\n",
    "\n",
    "    'an embroidered {}.',\n",
    "    'the embroidered {}.',\n",
    "\n",
    "    'a painting of the {}.',\n",
    "    'a painting of a {}.',\n",
    "]\n",
    "\n",
    "def build_text_embedding(categories):\n",
    "    if FLAGS.prompt_engineering:\n",
    "        templates = multiple_templates\n",
    "    else:\n",
    "        templates = single_template\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_text_embeddings = []\n",
    "        print(\"Building text embeddings...\")\n",
    "        \n",
    "        # Process in smaller batches\n",
    "        batch_size = 32  # Adjust this based on your GPU memory\n",
    "        for i in range(0, len(categories), batch_size):\n",
    "            batch_categories = categories[i:i + batch_size]\n",
    "            batch_embeddings = []\n",
    "            \n",
    "            for category in tqdm(batch_categories):\n",
    "                texts = [\n",
    "                    template.format(processed_name(category[\"name\"], rm_dot=True),\n",
    "                                  article=article(category[\"name\"]))\n",
    "                    for template in templates]\n",
    "                if FLAGS.this_is:\n",
    "                    texts = [\n",
    "                        \"This is \" + text if text.startswith(\"a\") or text.startswith(\"the\") else text \n",
    "                        for text in texts\n",
    "                    ]\n",
    "                texts = clip.tokenize(texts).to(device)\n",
    "                \n",
    "                try:\n",
    "                    text_embeddings = clip_model.encode_text(texts)\n",
    "                    text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "                    text_embedding = text_embeddings.mean(dim=0)\n",
    "                    text_embedding /= text_embedding.norm()\n",
    "                    batch_embeddings.append(text_embedding)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"CUDA out of memory. Error: {e}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # Retry with smaller batch\n",
    "                    texts = texts[:len(texts)//2]\n",
    "                    text_embeddings = clip_model.encode_text(texts)\n",
    "                    text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "                    text_embedding = text_embeddings.mean(dim=0)\n",
    "                    text_embedding /= text_embedding.norm()\n",
    "                    batch_embeddings.append(text_embedding)\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if len(batch_embeddings) % 8 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            all_text_embeddings.extend(batch_embeddings)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "        all_text_embeddings = all_text_embeddings.to(device)\n",
    "    \n",
    "    return all_text_embeddings.cpu().numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FK5lnSQEiM4r"
   },
   "outputs": [],
   "source": [
    "#@markdown Load ViLD model.\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "session = tf.Session(graph=tf.Graph(), config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "saved_model_dir = \"./image_path_v2\"\n",
    "_ = tf.saved_model.loader.load(session, [\"serve\"], saved_model_dir)\n",
    "\n",
    "numbered_categories = [{\"name\": str(idx), \"id\": idx,} for idx in range(50)]\n",
    "numbered_category_indices = {cat[\"id\"]: cat for cat in numbered_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "gazf5PBkkzlK"
   },
   "outputs": [],
   "source": [
    "#@markdown Non-maximum suppression (NMS).\n",
    "def nms(dets, scores, thresh, max_dets=1000):\n",
    "  \"\"\"Non-maximum suppression.\n",
    "  Args:\n",
    "    dets: [N, 4]\n",
    "    scores: [N,]\n",
    "    thresh: iou threshold. Float\n",
    "    max_dets: int.\n",
    "  \"\"\"\n",
    "  y1 = dets[:, 0]\n",
    "  x1 = dets[:, 1]\n",
    "  y2 = dets[:, 2]\n",
    "  x2 = dets[:, 3]\n",
    "\n",
    "  areas = (x2 - x1) * (y2 - y1)\n",
    "  order = scores.argsort()[::-1]\n",
    "\n",
    "  keep = []\n",
    "  while order.size > 0 and len(keep) < max_dets:\n",
    "    i = order[0]\n",
    "    keep.append(i)\n",
    "\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "    w = np.maximum(0.0, xx2 - xx1)\n",
    "    h = np.maximum(0.0, yy2 - yy1)\n",
    "    intersection = w * h\n",
    "    overlap = intersection / (areas[i] + areas[order[1:]] - intersection + 1e-12)\n",
    "\n",
    "    inds = np.where(overlap <= thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "  return keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ViLD Result Visualization Cell\n",
    "- **Purpose**: Visualization tools for detection results\n",
    "- **Key Functions**:\n",
    " - `draw_bounding_box_on_image()`\n",
    " - `draw_mask_on_image_array()`\n",
    " - `visualize_boxes_and_labels_on_image_array()`\n",
    "- **Features**:\n",
    " - Bounding box drawing\n",
    " - Instance mask visualization \n",
    " - Label and score display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "JO63v4Xuk1N6"
   },
   "outputs": [],
   "source": [
    "#@markdown ViLD Result Visualization\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "\n",
    "STANDARD_COLORS = [\"White\"]\n",
    "# STANDARD_COLORS = [\n",
    "#     \"AliceBlue\", \"Chartreuse\", \"Aqua\", \"Aquamarine\", \"Azure\", \"Beige\", \"Bisque\",\n",
    "#     \"BlanchedAlmond\", \"BlueViolet\", \"BurlyWood\", \"CadetBlue\", \"AntiqueWhite\",\n",
    "#     \"Chocolate\", \"Coral\", \"CornflowerBlue\", \"Cornsilk\", \"Cyan\",\n",
    "#     \"DarkCyan\", \"DarkGoldenRod\", \"DarkGrey\", \"DarkKhaki\", \"DarkOrange\",\n",
    "#     \"DarkOrchid\", \"DarkSalmon\", \"DarkSeaGreen\", \"DarkTurquoise\", \"DarkViolet\",\n",
    "#     \"DeepPink\", \"DeepSkyBlue\", \"DodgerBlue\", \"FloralWhite\",\n",
    "#     \"ForestGreen\", \"Fuchsia\", \"Gainsboro\", \"GhostWhite\", \"Gold\", \"GoldenRod\",\n",
    "#     \"Salmon\", \"Tan\", \"HoneyDew\", \"HotPink\", \"Ivory\", \"Khaki\",\n",
    "#     \"Lavender\", \"LavenderBlush\", \"LawnGreen\", \"LemonChiffon\", \"LightBlue\",\n",
    "#     \"LightCoral\", \"LightCyan\", \"LightGoldenRodYellow\", \"LightGray\", \"LightGrey\",\n",
    "#     \"LightGreen\", \"LightPink\", \"LightSalmon\", \"LightSeaGreen\", \"LightSkyBlue\",\n",
    "#     \"LightSlateGray\", \"LightSlateGrey\", \"LightSteelBlue\", \"LightYellow\", \"Lime\",\n",
    "#     \"LimeGreen\", \"Linen\", \"Magenta\", \"MediumAquaMarine\", \"MediumOrchid\",\n",
    "#     \"MediumPurple\", \"MediumSeaGreen\", \"MediumSlateBlue\", \"MediumSpringGreen\",\n",
    "#     \"MediumTurquoise\", \"MediumVioletRed\", \"MintCream\", \"MistyRose\", \"Moccasin\",\n",
    "#     \"NavajoWhite\", \"OldLace\", \"Olive\", \"OliveDrab\", \"Orange\",\n",
    "#     \"Orchid\", \"PaleGoldenRod\", \"PaleGreen\", \"PaleTurquoise\", \"PaleVioletRed\",\n",
    "#     \"PapayaWhip\", \"PeachPuff\", \"Peru\", \"Pink\", \"Plum\", \"PowderBlue\", \"Purple\",\n",
    "#     \"RosyBrown\", \"RoyalBlue\", \"SaddleBrown\", \"Green\", \"SandyBrown\",\n",
    "#     \"SeaGreen\", \"SeaShell\", \"Sienna\", \"Silver\", \"SkyBlue\", \"SlateBlue\",\n",
    "#     \"SlateGray\", \"SlateGrey\", \"Snow\", \"SpringGreen\", \"SteelBlue\", \"GreenYellow\",\n",
    "#     \"Teal\", \"Thistle\", \"Tomato\", \"Turquoise\", \"Violet\", \"Wheat\", \"White\",\n",
    "#     \"WhiteSmoke\", \"Yellow\", \"YellowGreen\"\n",
    "# ]\n",
    "\n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color=\"red\",\n",
    "                               thickness=4,\n",
    "                               display_str_list=(),\n",
    "                               use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "\n",
    "  Each string in display_str_list is displayed on a separate line above the\n",
    "  bounding box in black text on a rectangle filled with the input \"color\".\n",
    "  If the top of the bounding box extends to the edge of the image, the strings\n",
    "  are displayed below the bounding box.\n",
    "\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: list of strings to display in box\n",
    "                      (each to be shown on its own line).\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "  try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 24)\n",
    "  except IOError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  # If the total height of the display strings added to the top of the bounding\n",
    "  # box exceeds the top of the image, stack the strings below the bounding box\n",
    "  # instead of above.\n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  # Each display_str has a top and bottom margin of 0.05x.\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = bottom + total_display_str_height\n",
    "  # Reverse list and print from bottom to top.\n",
    "  for display_str in display_str_list[::-1]:\n",
    "    text_left = min(5, left)\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle(\n",
    "        [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n",
    "                                                          text_bottom)],\n",
    "        fill=color)\n",
    "    draw.text(\n",
    "        (left + margin, text_bottom - text_height - margin),\n",
    "        display_str,\n",
    "        fill=\"black\",\n",
    "        font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "def draw_bounding_box_on_image_array(image,\n",
    "                                     ymin,\n",
    "                                     xmin,\n",
    "                                     ymax,\n",
    "                                     xmax,\n",
    "                                     color=\"red\",\n",
    "                                     thickness=4,\n",
    "                                     display_str_list=(),\n",
    "                                     use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image (numpy array).\n",
    "\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "\n",
    "  Args:\n",
    "    image: a numpy array with shape [height, width, 3].\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: list of strings to display in box\n",
    "                      (each to be shown on its own line).\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
    "  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color,\n",
    "                             thickness, display_str_list,\n",
    "                             use_normalized_coordinates)\n",
    "  np.copyto(image, np.array(image_pil))\n",
    "\n",
    "\n",
    "def draw_mask_on_image_array(image, mask, color=\"red\", alpha=0.4):\n",
    "  \"\"\"Draws mask on an image.\n",
    "\n",
    "  Args:\n",
    "    image: uint8 numpy array with shape (img_height, img_height, 3)\n",
    "    mask: a uint8 numpy array of shape (img_height, img_height) with\n",
    "      values between either 0 or 1.\n",
    "    color: color to draw the keypoints with. Default is red.\n",
    "    alpha: transparency value between 0 and 1. (default: 0.4)\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On incorrect data type for image or masks.\n",
    "  \"\"\"\n",
    "  if image.dtype != np.uint8:\n",
    "    raise ValueError(\"`image` not of type np.uint8\")\n",
    "  if mask.dtype != np.uint8:\n",
    "    raise ValueError(\"`mask` not of type np.uint8\")\n",
    "  if np.any(np.logical_and(mask != 1, mask != 0)):\n",
    "    raise ValueError(\"`mask` elements should be in [0, 1]\")\n",
    "  if image.shape[:2] != mask.shape:\n",
    "    raise ValueError(\"The image has spatial dimensions %s but the mask has \"\n",
    "                     \"dimensions %s\" % (image.shape[:2], mask.shape))\n",
    "  rgb = ImageColor.getrgb(color)\n",
    "  pil_image = Image.fromarray(image)\n",
    "\n",
    "  solid_color = np.expand_dims(\n",
    "      np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert(\"RGBA\")\n",
    "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert(\"L\")\n",
    "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "  np.copyto(image, np.array(pil_image.convert(\"RGB\")))\n",
    "\n",
    "def visualize_boxes_and_labels_on_image_array(\n",
    "    image,\n",
    "    boxes,\n",
    "    classes,\n",
    "    scores,\n",
    "    category_index,\n",
    "    instance_masks=None,\n",
    "    instance_boundaries=None,\n",
    "    use_normalized_coordinates=False,\n",
    "    max_boxes_to_draw=20,\n",
    "    min_score_thresh=.5,\n",
    "    agnostic_mode=False,\n",
    "    line_thickness=1,\n",
    "    groundtruth_box_visualization_color=\"black\",\n",
    "    skip_scores=False,\n",
    "    skip_labels=False,\n",
    "    mask_alpha=0.4,\n",
    "    plot_color=None,\n",
    "):\n",
    "  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\n",
    "\n",
    "  This function groups boxes that correspond to the same location\n",
    "  and creates a display string for each detection and overlays these\n",
    "  on the image. Note that this function modifies the image in place, and returns\n",
    "  that same image.\n",
    "\n",
    "  Args:\n",
    "    image: uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    boxes: a numpy array of shape [N, 4]\n",
    "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
    "      and match the keys in the label map.\n",
    "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
    "      this function assumes that the boxes to be plotted are groundtruth\n",
    "      boxes and plot all boxes as black with no classes or scores.\n",
    "    category_index: a dict containing category dictionaries (each holding\n",
    "      category index `id` and category name `name`) keyed by category indices.\n",
    "    instance_masks: a numpy array of shape [N, image_height, image_width] with\n",
    "      values ranging between 0 and 1, can be None.\n",
    "    instance_boundaries: a numpy array of shape [N, image_height, image_width]\n",
    "      with values ranging between 0 and 1, can be None.\n",
    "    use_normalized_coordinates: whether boxes is to be interpreted as\n",
    "      normalized coordinates or not.\n",
    "    max_boxes_to_draw: maximum number of boxes to visualize.  If None, draw\n",
    "      all boxes.\n",
    "    min_score_thresh: minimum score threshold for a box to be visualized\n",
    "    agnostic_mode: boolean (default: False) controlling whether to evaluate in\n",
    "      class-agnostic mode or not.  This mode will display scores but ignore\n",
    "      classes.\n",
    "    line_thickness: integer (default: 4) controlling line width of the boxes.\n",
    "    groundtruth_box_visualization_color: box color for visualizing groundtruth\n",
    "      boxes\n",
    "    skip_scores: whether to skip score when drawing a single detection\n",
    "    skip_labels: whether to skip label when drawing a single detection\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3) with overlaid boxes.\n",
    "  \"\"\"\n",
    "  # Create a display string (and color) for every box location, group any boxes\n",
    "  # that correspond to the same location.\n",
    "  box_to_display_str_map = collections.defaultdict(list)\n",
    "  box_to_color_map = collections.defaultdict(str)\n",
    "  box_to_instance_masks_map = {}\n",
    "  box_to_score_map = {}\n",
    "  box_to_instance_boundaries_map = {}\n",
    "  \n",
    "  if not max_boxes_to_draw:\n",
    "    max_boxes_to_draw = boxes.shape[0]\n",
    "  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "    if scores is None or scores[i] > min_score_thresh:\n",
    "      box = tuple(boxes[i].tolist())\n",
    "      if instance_masks is not None:\n",
    "        box_to_instance_masks_map[box] = instance_masks[i]\n",
    "      if instance_boundaries is not None:\n",
    "        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n",
    "      if scores is None:\n",
    "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
    "      else:\n",
    "        display_str = \"\"\n",
    "        if not skip_labels:\n",
    "          if not agnostic_mode:\n",
    "            if classes[i] in list(category_index.keys()):\n",
    "              class_name = category_index[classes[i]][\"name\"]\n",
    "            else:\n",
    "              class_name = \"N/A\"\n",
    "            display_str = str(class_name)\n",
    "        if not skip_scores:\n",
    "          if not display_str:\n",
    "            display_str = \"{}%\".format(int(100*scores[i]))\n",
    "          else:\n",
    "            float_score = (\"%.2f\" % scores[i]).lstrip(\"0\")\n",
    "            display_str = \"{}: {}\".format(display_str, float_score)\n",
    "          box_to_score_map[box] = int(100*scores[i])\n",
    "\n",
    "        box_to_display_str_map[box].append(display_str)\n",
    "        if plot_color is not None:\n",
    "          box_to_color_map[box] = plot_color\n",
    "        elif agnostic_mode:\n",
    "          box_to_color_map[box] = \"DarkOrange\"\n",
    "        else:\n",
    "          box_to_color_map[box] = STANDARD_COLORS[\n",
    "              classes[i] % len(STANDARD_COLORS)]\n",
    "\n",
    "  # Handle the case when box_to_score_map is empty.\n",
    "  if box_to_score_map:\n",
    "    box_color_iter = sorted(\n",
    "        box_to_color_map.items(), key=lambda kv: box_to_score_map[kv[0]])\n",
    "  else:\n",
    "    box_color_iter = box_to_color_map.items()\n",
    "\n",
    "  # Draw all boxes onto image.\n",
    "  for box, color in box_color_iter:\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    if instance_masks is not None:\n",
    "      draw_mask_on_image_array(\n",
    "          image,\n",
    "          box_to_instance_masks_map[box],\n",
    "          color=color,\n",
    "          alpha=mask_alpha\n",
    "      )\n",
    "    if instance_boundaries is not None:\n",
    "      draw_mask_on_image_array(\n",
    "          image,\n",
    "          box_to_instance_boundaries_map[box],\n",
    "          color=\"red\",\n",
    "          alpha=1.0\n",
    "      )\n",
    "    draw_bounding_box_on_image_array(\n",
    "        image,\n",
    "        ymin,\n",
    "        xmin,\n",
    "        ymax,\n",
    "        xmax,\n",
    "        color=color,\n",
    "        thickness=line_thickness,\n",
    "        display_str_list=box_to_display_str_map[box],\n",
    "        use_normalized_coordinates=use_normalized_coordinates)\n",
    "    \n",
    "  return image\n",
    "\n",
    "\n",
    "def paste_instance_masks(masks,\n",
    "                         detected_boxes,\n",
    "                         image_height,\n",
    "                         image_width):\n",
    "  \"\"\"Paste instance masks to generate the image segmentation results.\n",
    "\n",
    "  Args:\n",
    "    masks: a numpy array of shape [N, mask_height, mask_width] representing the\n",
    "      instance masks w.r.t. the `detected_boxes`.\n",
    "    detected_boxes: a numpy array of shape [N, 4] representing the reference\n",
    "      bounding boxes.\n",
    "    image_height: an integer representing the height of the image.\n",
    "    image_width: an integer representing the width of the image.\n",
    "\n",
    "  Returns:\n",
    "    segms: a numpy array of shape [N, image_height, image_width] representing\n",
    "      the instance masks *pasted* on the image canvas.\n",
    "  \"\"\"\n",
    "\n",
    "  def expand_boxes(boxes, scale):\n",
    "    \"\"\"Expands an array of boxes by a given scale.\"\"\"\n",
    "    # Reference: https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/boxes.py#L227  # pylint: disable=line-too-long\n",
    "    # The `boxes` in the reference implementation is in [x1, y1, x2, y2] form,\n",
    "    # whereas `boxes` here is in [x1, y1, w, h] form\n",
    "    w_half = boxes[:, 2] * .5\n",
    "    h_half = boxes[:, 3] * .5\n",
    "    x_c = boxes[:, 0] + w_half\n",
    "    y_c = boxes[:, 1] + h_half\n",
    "\n",
    "    w_half *= scale\n",
    "    h_half *= scale\n",
    "\n",
    "    boxes_exp = np.zeros(boxes.shape)\n",
    "    boxes_exp[:, 0] = x_c - w_half\n",
    "    boxes_exp[:, 2] = x_c + w_half\n",
    "    boxes_exp[:, 1] = y_c - h_half\n",
    "    boxes_exp[:, 3] = y_c + h_half\n",
    "\n",
    "    return boxes_exp\n",
    "\n",
    "  # Reference: https://github.com/facebookresearch/Detectron/blob/master/detectron/core/test.py#L812  # pylint: disable=line-too-long\n",
    "  # To work around an issue with cv2.resize (it seems to automatically pad\n",
    "  # with repeated border values), we manually zero-pad the masks by 1 pixel\n",
    "  # prior to resizing back to the original image resolution. This prevents\n",
    "  # \"top hat\" artifacts. We therefore need to expand the reference boxes by an\n",
    "  # appropriate factor.\n",
    "  _, mask_height, mask_width = masks.shape\n",
    "  scale = max((mask_width + 2.0) / mask_width,\n",
    "              (mask_height + 2.0) / mask_height)\n",
    "\n",
    "  ref_boxes = expand_boxes(detected_boxes, scale)\n",
    "  ref_boxes = ref_boxes.astype(np.int32)\n",
    "  padded_mask = np.zeros((mask_height + 2, mask_width + 2), dtype=np.float32)\n",
    "  segms = []\n",
    "  for mask_ind, mask in enumerate(masks):\n",
    "    im_mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "    # Process mask inside bounding boxes.\n",
    "    padded_mask[1:-1, 1:-1] = mask[:, :]\n",
    "\n",
    "    ref_box = ref_boxes[mask_ind, :]\n",
    "    w = ref_box[2] - ref_box[0] + 1\n",
    "    h = ref_box[3] - ref_box[1] + 1\n",
    "    w = np.maximum(w, 1)\n",
    "    h = np.maximum(h, 1)\n",
    "\n",
    "    mask = cv2.resize(padded_mask, (w, h))\n",
    "    mask = np.array(mask > 0.5, dtype=np.uint8)\n",
    "\n",
    "    x_0 = min(max(ref_box[0], 0), image_width)\n",
    "    x_1 = min(max(ref_box[2] + 1, 0), image_width)\n",
    "    y_0 = min(max(ref_box[1], 0), image_height)\n",
    "    y_1 = min(max(ref_box[3] + 1, 0), image_height)\n",
    "\n",
    "    im_mask[y_0:y_1, x_0:x_1] = mask[\n",
    "        (y_0 - ref_box[1]):(y_1 - ref_box[1]),\n",
    "        (x_0 - ref_box[0]):(x_1 - ref_box[0])\n",
    "    ]\n",
    "    segms.append(im_mask)\n",
    "\n",
    "  segms = np.array(segms)\n",
    "  assert masks.shape[0] == segms.shape[0]\n",
    "  return segms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "kKKH_wTak3F_"
   },
   "outputs": [],
   "source": [
    "#@markdown Plot instance masks.\n",
    "def plot_mask(color, alpha, original_image, mask):\n",
    "  rgb = ImageColor.getrgb(color)\n",
    "  pil_image = Image.fromarray(original_image)\n",
    "\n",
    "  solid_color = np.expand_dims(\n",
    "      np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert(\"RGBA\")\n",
    "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert(\"L\")\n",
    "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "  img_w_mask = np.array(pil_image.convert(\"RGB\"))\n",
    "  return img_w_mask\n",
    "\n",
    "%matplotlib inline\n",
    "def display_image(path_or_array, size=(10, 10)):\n",
    "  if isinstance(path_or_array, str):\n",
    "    image = np.asarray(Image.open(open(image_path, \"rb\")).convert(\"RGB\"))\n",
    "  else:\n",
    "    image = path_or_array\n",
    "  \n",
    "  plt.figure(figsize=size)\n",
    "  plt.imshow(image)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ViLD Forward Pass Cell\n",
    "- **Purpose**: Main model execution logic\n",
    "- **Key Steps**:\n",
    " 1. Preprocess categories and parameters\n",
    " 2. Run object detection\n",
    " 3. Filter and process boxes\n",
    " 4. Compute text embeddings\n",
    " 5. Match detections with categories\n",
    " 6. Return found objects and visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "X2-SsyEolAr8"
   },
   "outputs": [],
   "source": [
    "#@markdown Define ViLD forward pass.\n",
    "\n",
    "def vild(image_path, category_name_string, params, plot_on=True, prompt_swaps=[]):\n",
    "  #################################################################\n",
    "  # Preprocessing categories and get params\n",
    "  for a, b in prompt_swaps:\n",
    "    category_name_string = category_name_string.replace(a, b)\n",
    "  category_names = [x.strip() for x in category_name_string.split(\";\")]\n",
    "  category_names = [\"background\"] + category_names\n",
    "  categories = [{\"name\": item, \"id\": idx+1,} for idx, item in enumerate(category_names)]\n",
    "  category_indices = {cat[\"id\"]: cat for cat in categories}\n",
    "  \n",
    "  max_boxes_to_draw, nms_threshold, min_rpn_score_thresh, min_box_area, max_box_area = params\n",
    "  fig_size_h = min(max(5, int(len(category_names) / 2.5) ), 10)\n",
    "\n",
    "\n",
    "  #################################################################\n",
    "  # Obtain results and read image\n",
    "  roi_boxes, roi_scores, detection_boxes, scores_unused, box_outputs, detection_masks, visual_features, image_info = session.run(\n",
    "        [\"RoiBoxes:0\", \"RoiScores:0\", \"2ndStageBoxes:0\", \"2ndStageScoresUnused:0\", \"BoxOutputs:0\", \"MaskOutputs:0\", \"VisualFeatOutputs:0\", \"ImageInfo:0\"],\n",
    "        feed_dict={\"Placeholder:0\": [image_path,]})\n",
    "  \n",
    "  roi_boxes = np.squeeze(roi_boxes, axis=0)  # squeeze\n",
    "  # no need to clip the boxes, already done\n",
    "  roi_scores = np.squeeze(roi_scores, axis=0)\n",
    "\n",
    "  detection_boxes = np.squeeze(detection_boxes, axis=(0, 2))\n",
    "  scores_unused = np.squeeze(scores_unused, axis=0)\n",
    "  box_outputs = np.squeeze(box_outputs, axis=0)\n",
    "  detection_masks = np.squeeze(detection_masks, axis=0)\n",
    "  visual_features = np.squeeze(visual_features, axis=0)\n",
    "\n",
    "  image_info = np.squeeze(image_info, axis=0)  # obtain image info\n",
    "  image_scale = np.tile(image_info[2:3, :], (1, 2))\n",
    "  image_height = int(image_info[0, 0])\n",
    "  image_width = int(image_info[0, 1])\n",
    "\n",
    "  rescaled_detection_boxes = detection_boxes / image_scale # rescale\n",
    "\n",
    "  # Read image\n",
    "  image = np.asarray(Image.open(open(image_path, \"rb\")).convert(\"RGB\"))\n",
    "  assert image_height == image.shape[0]\n",
    "  assert image_width == image.shape[1]\n",
    "\n",
    "\n",
    "  #################################################################\n",
    "  # Filter boxes\n",
    "\n",
    "  # Apply non-maximum suppression to detected boxes with nms threshold.\n",
    "  nmsed_indices = nms(\n",
    "      detection_boxes,\n",
    "      roi_scores,\n",
    "      thresh=nms_threshold\n",
    "      )\n",
    "\n",
    "  # Compute RPN box size.\n",
    "  box_sizes = (rescaled_detection_boxes[:, 2] - rescaled_detection_boxes[:, 0]) * (rescaled_detection_boxes[:, 3] - rescaled_detection_boxes[:, 1])\n",
    "\n",
    "  # Filter out invalid rois (nmsed rois)\n",
    "  valid_indices = np.where(\n",
    "      np.logical_and(\n",
    "        np.isin(np.arange(len(roi_scores), dtype=np.int32), nmsed_indices),\n",
    "        np.logical_and(\n",
    "            np.logical_not(np.all(roi_boxes == 0., axis=-1)),\n",
    "            np.logical_and(\n",
    "              roi_scores >= min_rpn_score_thresh,\n",
    "              np.logical_and(\n",
    "                box_sizes > min_box_area,\n",
    "                box_sizes < max_box_area\n",
    "                )\n",
    "              )\n",
    "        )    \n",
    "      )\n",
    "  )[0]\n",
    "\n",
    "  detection_roi_scores = roi_scores[valid_indices][:max_boxes_to_draw, ...]\n",
    "  detection_boxes = detection_boxes[valid_indices][:max_boxes_to_draw, ...]\n",
    "  detection_masks = detection_masks[valid_indices][:max_boxes_to_draw, ...]\n",
    "  detection_visual_feat = visual_features[valid_indices][:max_boxes_to_draw, ...]\n",
    "  rescaled_detection_boxes = rescaled_detection_boxes[valid_indices][:max_boxes_to_draw, ...]\n",
    "\n",
    "\n",
    "  #################################################################\n",
    "  # Compute text embeddings and detection scores, and rank results\n",
    "  text_features = build_text_embedding(categories)\n",
    "  \n",
    "  raw_scores = detection_visual_feat.dot(text_features.T)\n",
    "  if FLAGS.use_softmax:\n",
    "    scores_all = softmax(FLAGS.temperature * raw_scores, axis=-1)\n",
    "  else:\n",
    "    scores_all = raw_scores\n",
    "\n",
    "  indices = np.argsort(-np.max(scores_all, axis=1))  # Results are ranked by scores\n",
    "  indices_fg = np.array([i for i in indices if np.argmax(scores_all[i]) != 0])\n",
    "\n",
    "  \n",
    "  #################################################################\n",
    "  # Print found_objects\n",
    "  found_objects = []\n",
    "  for a, b in prompt_swaps:\n",
    "    category_names = [name.replace(b, a) for name in category_names]  # Extra prompt engineering.\n",
    "  for anno_idx in indices[0:int(rescaled_detection_boxes.shape[0])]:\n",
    "    scores = scores_all[anno_idx]\n",
    "    if np.argmax(scores) == 0:\n",
    "      continue\n",
    "    found_object = category_names[np.argmax(scores)]\n",
    "    if found_object == \"background\":\n",
    "      continue\n",
    "    print(\"Found a\", found_object, \"with score:\", np.max(scores))\n",
    "    found_objects.append(category_names[np.argmax(scores)])\n",
    "  if not plot_on:\n",
    "    return found_objects\n",
    "  \n",
    "\n",
    "  #################################################################\n",
    "  # Plot detected boxes on the input image.\n",
    "  ymin, xmin, ymax, xmax = np.split(rescaled_detection_boxes, 4, axis=-1)\n",
    "  processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)\n",
    "  segmentations = paste_instance_masks(detection_masks, processed_boxes, image_height, image_width)\n",
    "\n",
    "  if len(indices_fg) == 0:\n",
    "    display_image(np.array(image), size=overall_fig_size)\n",
    "    print(\"ViLD does not detect anything belong to the given category\")\n",
    "\n",
    "  else:\n",
    "    image_with_detections = visualize_boxes_and_labels_on_image_array(\n",
    "        np.array(image),\n",
    "        rescaled_detection_boxes[indices_fg],\n",
    "        valid_indices[:max_boxes_to_draw][indices_fg],\n",
    "        detection_roi_scores[indices_fg],    \n",
    "        numbered_category_indices,\n",
    "        instance_masks=segmentations[indices_fg],\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=max_boxes_to_draw,\n",
    "        min_score_thresh=min_rpn_score_thresh,\n",
    "        skip_scores=False,\n",
    "        skip_labels=True)\n",
    "\n",
    "    # plt.figure(figsize=overall_fig_size)\n",
    "    plt.imshow(image_with_detections)\n",
    "    # plt.axis(\"off\")\n",
    "    plt.title(\"ViLD detected objects and RPN scores.\")\n",
    "    plt.show()\n",
    "\n",
    "  return found_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jwAERXZl2Yl"
   },
   "outputs": [],
   "source": [
    "category_names = ['blue block',\n",
    "                  'red block',\n",
    "                  'green block',\n",
    "                  'orange block',\n",
    "                  'yellow block',\n",
    "                  'purple block',\n",
    "                  'pink block',\n",
    "                  'cyan block',\n",
    "                  'brown block',\n",
    "                  'gray block',\n",
    "\n",
    "                  'blue bowl',\n",
    "                  'red bowl',\n",
    "                  'green bowl',\n",
    "                  'orange bowl',\n",
    "                  'yellow bowl',\n",
    "                  'purple bowl',\n",
    "                  'pink bowl',\n",
    "                  'cyan bowl',\n",
    "                  'brown bowl',\n",
    "                  'gray bowl']\n",
    "image_path = 'tmp.jpg'\n",
    "\n",
    "#@markdown ViLD settings.\n",
    "category_name_string = \";\".join(category_names)\n",
    "max_boxes_to_draw = 8 #@param {type:\"integer\"}\n",
    "\n",
    "# Extra prompt engineering: swap A with B for every (A, B) in list.\n",
    "prompt_swaps = [('block', 'cube')]\n",
    "\n",
    "nms_threshold = 0.4 #@param {type:\"slider\", min:0, max:0.9, step:0.05}\n",
    "min_rpn_score_thresh = 0.4  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "min_box_area = 10 #@param {type:\"slider\", min:0, max:10000, step:1.0}\n",
    "max_box_area = 3000  #@param {type:\"slider\", min:0, max:10000, step:1.0}\n",
    "vild_params = max_boxes_to_draw, nms_threshold, min_rpn_score_thresh, min_box_area, max_box_area\n",
    "found_objects = vild(image_path, category_name_string, vild_params, plot_on=True, prompt_swaps=prompt_swaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDM9CH_qXGKH"
   },
   "source": [
    "### **Scripted Expert**\n",
    "Scripted pick and place oracle to collect expert demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oOcKdshBXGKH"
   },
   "outputs": [],
   "source": [
    "class ScriptedPolicy():\n",
    "\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "\n",
    "  def step(self, text, obs):\n",
    "    print(f'Input: {text}')\n",
    "\n",
    "    # Parse pick and place targets.\n",
    "    pick_text, place_text = text.split('and')\n",
    "    pick_target, place_target = None, None\n",
    "    for name in PICK_TARGETS.keys():\n",
    "      if name in pick_text:\n",
    "        pick_target = name\n",
    "        break\n",
    "    for name in PLACE_TARGETS.keys():\n",
    "      if name in place_text:\n",
    "        place_target = name\n",
    "        break\n",
    "\n",
    "    # Admissable targets only.\n",
    "    assert pick_target is not None\n",
    "    assert place_target is not None\n",
    "\n",
    "    pick_id = self.env.obj_name_to_id[pick_target]\n",
    "    pick_pose = pybullet.getBasePositionAndOrientation(pick_id)\n",
    "    pick_position = np.float32(pick_pose[0])\n",
    "\n",
    "    if place_target in self.env.obj_name_to_id:\n",
    "      place_id = self.env.obj_name_to_id[place_target]\n",
    "      place_pose = pybullet.getBasePositionAndOrientation(place_id)\n",
    "      place_position = np.float32(place_pose[0])\n",
    "    else:\n",
    "      place_position = np.float32(PLACE_TARGETS[place_target])\n",
    "\n",
    "    # Add some noise to pick and place positions.\n",
    "    # pick_position[:2] += np.random.normal(scale=0.01)\n",
    "    place_position[:2] += np.random.normal(scale=0.01)\n",
    "\n",
    "    act = {'pick': pick_position, 'place': place_position}\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCvl1WmvW3lK"
   },
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lKMSa_ryW3lL"
   },
   "outputs": [],
   "source": [
    "#@markdown Collect demonstrations with a scripted expert, or download a pre-generated dataset.\n",
    "load_pregenerated = True  #@param {type:\"boolean\"}\n",
    "\n",
    "# Load pre-existing dataset.\n",
    "if load_pregenerated:\n",
    "  if not os.path.exists('dataset-9999.pkl'):\n",
    "    # !gdown --id 1TECwTIfawxkRYbzlAey0z1mqXKcyfPc-\n",
    "    !gdown --id 1yCz6C-6eLWb4SFYKdkM-wz5tlMjbG2h8\n",
    "  dataset = pickle.load(open('dataset-9999.pkl', 'rb'))  # ~10K samples.\n",
    "  dataset_size = len(dataset['text'])\n",
    "\n",
    "# Generate new dataset.\n",
    "else:\n",
    "  dataset = {}\n",
    "  dataset_size = 2  # Size of new dataset.\n",
    "  dataset['image'] = np.zeros((dataset_size, 224, 224, 3), dtype=np.uint8)\n",
    "  dataset['pick_yx'] = np.zeros((dataset_size, 2), dtype=np.int32)\n",
    "  dataset['place_yx'] = np.zeros((dataset_size, 2), dtype=np.int32)\n",
    "  dataset['text'] = []\n",
    "  policy = ScriptedPolicy(env)\n",
    "  data_idx = 0\n",
    "  while data_idx < dataset_size:\n",
    "    np.random.seed(data_idx)\n",
    "    num_pick, num_place = 3, 3\n",
    "\n",
    "    # Select random objects for data collection.\n",
    "    pick_items = list(PICK_TARGETS.keys())\n",
    "    pick_items = np.random.choice(pick_items, size=num_pick, replace=False)\n",
    "    place_items = list(PLACE_TARGETS.keys())\n",
    "    for pick_item in pick_items:  # For simplicity: place items != pick items.\n",
    "      place_items.remove(pick_item)\n",
    "    place_items = np.random.choice(place_items, size=num_place, replace=False)\n",
    "    config = {'pick': pick_items, 'place': place_items}\n",
    "\n",
    "    # Initialize environment with selected objects.\n",
    "    obs = env.reset(config)\n",
    "\n",
    "    # Create text prompts.\n",
    "    prompts = []\n",
    "    for i in range(len(pick_items)):\n",
    "      pick_item = pick_items[i]\n",
    "      place_item = place_items[i]\n",
    "      prompts.append(f'Pick the {pick_item} and place it on the {place_item}.')\n",
    "\n",
    "    # Execute 3 pick and place actions.\n",
    "    for prompt in prompts:\n",
    "      act = policy.step(prompt, obs)\n",
    "      dataset['text'].append(prompt)\n",
    "      dataset['image'][data_idx, ...] = obs['image'].copy()\n",
    "      dataset['pick_yx'][data_idx, ...] = xyz_to_pix(act['pick'])\n",
    "      dataset['place_yx'][data_idx, ...] = xyz_to_pix(act['place'])\n",
    "      data_idx += 1\n",
    "      obs, _, _, _ = env.step(act)\n",
    "      debug_clip = ImageSequenceClip(env.cache_video, fps=25)\n",
    "      display(debug_clip.ipython_display(autoplay=1, loop=1))\n",
    "      env.cache_video = []\n",
    "      if data_idx >= dataset_size:\n",
    "        break\n",
    "\n",
    "  pickle.dump(dataset, open(f'dataset-{dataset_size}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-wVcfLWlYm73"
   },
   "outputs": [],
   "source": [
    "#@markdown Show a demonstration example from the dataset.\n",
    "\n",
    "img = dataset['image'][0]\n",
    "pick_yx = dataset['pick_yx'][0]\n",
    "place_yx = dataset['place_yx'][0]\n",
    "text = dataset['text'][0]\n",
    "plt.title(text)\n",
    "plt.imshow(img)\n",
    "plt.arrow(pick_yx[1], pick_yx[0], place_yx[1]-pick_yx[1], place_yx[0]-pick_yx[0], color='w', head_starts_at_zero=False, head_width=7, length_includes_head=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SayCan Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Monitoring Setup\n",
    "\n",
    "This section sets up tracking for:\n",
    "- API call latency\n",
    "- Token usage and costs\n",
    "- Success/failure rates\n",
    "- Resource utilization\n",
    "\n",
    "The PerformanceMonitor class handles metrics collection for the language model interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict # Default dictionary for metrics\n",
    "\n",
    "#@title Performance Monitoring Setup\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.total_cost = 0\n",
    "        # GPT-3.5-turbo-instruct pricing per 1K tokens\n",
    "        self.price_per_token = {\n",
    "            'prompt': 0.0015 / 1000,  # Input tokens\n",
    "            'completion': 0.002 / 1000 # Output tokens\n",
    "        }\n",
    "    \n",
    "    def log_api_call(self, start_time, response, call_type=\"scoring\"):\n",
    "        duration = time.time() - start_time\n",
    "        prompt_tokens = response['usage']['prompt_tokens']\n",
    "        completion_tokens = response['usage']['completion_tokens']\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "        \n",
    "        # Calculate cost\n",
    "        call_cost = (prompt_tokens * self.price_per_token['prompt'] + \n",
    "                    completion_tokens * self.price_per_token['completion'])\n",
    "        self.total_cost += call_cost\n",
    "        \n",
    "        self.metrics['duration'].append(duration)\n",
    "        self.metrics['prompt_tokens'].append(prompt_tokens)\n",
    "        self.metrics['completion_tokens'].append(completion_tokens)\n",
    "        self.metrics['total_tokens'].append(total_tokens)\n",
    "        self.metrics['cost'].append(call_cost)\n",
    "        self.metrics['call_type'].append(call_type)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            'total_calls': len(self.metrics['duration']),\n",
    "            'total_cost': self.total_cost,\n",
    "            'avg_duration': sum(self.metrics['duration']) / len(self.metrics['duration']),\n",
    "            'total_tokens': sum(self.metrics['total_tokens']),\n",
    "            'avg_tokens_per_call': sum(self.metrics['total_tokens']) / len(self.metrics['total_tokens'])\n",
    "        }\n",
    "    \n",
    "    def print_summary(self):\n",
    "        summary = self.get_summary()\n",
    "        print(f\"Performance Summary:\")\n",
    "        print(f\"Total API Calls: {summary['total_calls']}\")\n",
    "        print(f\"Total Cost: ${summary['total_cost']:.4f}\")\n",
    "        print(f\"Average Duration: {summary['avg_duration']:.2f} seconds\")\n",
    "        print(f\"Total Tokens Used: {summary['total_tokens']}\")\n",
    "        print(f\"Average Tokens per Call: {summary['avg_tokens_per_call']:.1f}\")\n",
    "\n",
    "performance_monitor = PerformanceMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Tracking Setup\n",
    "\n",
    "This section implements tracking for:\n",
    "- Plan generation success rates\n",
    "- Execution success rates\n",
    "- Historical record of plans and executions\n",
    "- Plan-to-execution correlation\n",
    "\n",
    "The ExecutionTracker class maintains metrics about the robot's performance in carrying out generated plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Execution Tracking Setup\n",
    "class ExecutionTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'total_plans': 0,\n",
    "            'successful_plans': 0,\n",
    "            'total_executions': 0,\n",
    "            'successful_executions': 0,\n",
    "            'plan_history': [],\n",
    "            'execution_history': []\n",
    "        }\n",
    "    \n",
    "    def log_plan(self, instruction, plan, success):\n",
    "        self.metrics['total_plans'] += 1\n",
    "        if success:\n",
    "            self.metrics['successful_plans'] += 1\n",
    "        self.metrics['plan_history'].append({\n",
    "            'instruction': instruction,\n",
    "            'plan': plan,\n",
    "            'success': success\n",
    "        })\n",
    "    \n",
    "    def log_execution(self, instruction, plan, success):\n",
    "        self.metrics['total_executions'] += 1\n",
    "        if success:\n",
    "            self.metrics['successful_executions'] += 1\n",
    "        self.metrics['execution_history'].append({\n",
    "            'instruction': instruction,\n",
    "            'plan': plan,\n",
    "            'success': success\n",
    "        })\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(f\"Execution Summary:\")\n",
    "        print(f\"Planning Success Rate: {self.metrics['successful_plans']/max(1,self.metrics['total_plans'])*100:.1f}%\")\n",
    "        print(f\"Execution Success Rate: {self.metrics['successful_executions']/max(1,self.metrics['total_executions'])*100:.1f}%\")\n",
    "\n",
    "execution_tracker = ExecutionTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Scoring Functions\n",
    "Implements GPT-3 based scoring for action selection:\n",
    "- Option generation\n",
    "- Action scoring\n",
    "- Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Cn53B4YsII3a"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "#@title LLM Cache\n",
    "overwrite_cache = True\n",
    "if overwrite_cache:\n",
    "  LLM_CACHE = {}\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "def gpt3_call(engine=\"gpt-3.5-turbo-instruct\", prompt=\"\", max_tokens=128, temperature=0):\n",
    "    \"\"\"Updated GPT-3 call using new OpenAI API format\"\"\"\n",
    "    full_query = \"\"\n",
    "    for p in prompt:\n",
    "        full_query += p\n",
    "    id = tuple((engine, full_query, max_tokens, temperature))\n",
    "    \n",
    "    if id in LLM_CACHE.keys():\n",
    "        print('cache hit, returning')\n",
    "        response = LLM_CACHE[id]\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        response = client.completions.create(\n",
    "            model=engine,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        # Convert response to dict for compatibility with existing code\n",
    "        response_dict = {\n",
    "            \"choices\": [{\"text\": choice.text} for choice in response.choices],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                \"completion_tokens\": response.usage.completion_tokens,\n",
    "                \"total_tokens\": response.usage.total_tokens\n",
    "            }\n",
    "        }\n",
    "        performance_monitor.log_api_call(start_time, response_dict)\n",
    "        LLM_CACHE[id] = response_dict\n",
    "        response = response_dict\n",
    "    return response\n",
    "\n",
    "def gpt3_scoring(query, options, engine=\"gpt-3.5-turbo-instruct\", limit_num_options=None, \n",
    "                option_start=\"\\n\", verbose=False, print_tokens=False):\n",
    "    if limit_num_options:\n",
    "        options = options[:limit_num_options]\n",
    "    \n",
    "    gpt3_prompt_options = []\n",
    "    for option in options:\n",
    "        # Handle done() case first\n",
    "        if option == \"done()\":\n",
    "            prompt = f\"\"\"Task: {query}\n",
    "Current action to evaluate: {option}\n",
    "Rate if task is complete (0-10):\n",
    "0: Task is far from complete\n",
    "10: All required steps are done\n",
    "Provide score (0-10):\"\"\"\n",
    "            gpt3_prompt_options.append(prompt)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Safely parse pick and place objects\n",
    "            if \"pick_and_place\" in option:\n",
    "                parts = option.split(\"(\")[1].strip(\")\").split(\",\")\n",
    "                if len(parts) == 2:\n",
    "                    pick_obj = parts[0].strip()\n",
    "                    place_target = parts[1].strip()\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid option format: {option}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown option type: {option}\")\n",
    "\n",
    "            if \"corner\" in query.lower():\n",
    "                prompt = f\"\"\"Task: {query}\n",
    "Action: Pick {pick_obj} and place at {place_target}\n",
    "\n",
    "Rate this specific action considering:\n",
    "1. Does it place a block in a corner? (necessary for task)\n",
    "2. Is this the most efficient corner to use next?\n",
    "3. Will this placement block other required corner placements?\n",
    "\n",
    "Score meaning:\n",
    "0: Invalid or impossible action\n",
    "2: Places block in non-corner location\n",
    "4: Places in corner but creates inefficient distribution\n",
    "6: Valid corner placement but not optimal sequence\n",
    "8: Good corner placement enabling efficient completion\n",
    "10: Perfect next move for corner distribution\n",
    "\n",
    "Provide score (0-10):\"\"\"\n",
    "            \n",
    "            elif \"matching\" in query.lower() or \"colored\" in query.lower():\n",
    "                prompt = f\"\"\"Task: {query}\n",
    "Action: Pick {pick_obj} and place at {place_target}\n",
    "\n",
    "Rate this specific action considering:\n",
    "1. Does it match block color with bowl color?\n",
    "2. Is this the best color match to make now?\n",
    "3. Is this placement efficient for overall task?\n",
    "\n",
    "Score meaning:\n",
    "0: Invalid action\n",
    "2: Non-matching colors or wrong target\n",
    "4: Valid but inefficient color match\n",
    "6: Good color match but suboptimal timing\n",
    "8: Efficient color match\n",
    "10: Perfect color match and sequence\n",
    "\n",
    "Provide score (0-10):\"\"\"\n",
    "\n",
    "            else:\n",
    "                prompt = f\"\"\"Task: {query}\n",
    "Action: Pick {pick_obj} and place at {place_target}\n",
    "\n",
    "Rate this specific action considering:\n",
    "1. Does it directly help achieve the task?\n",
    "2. Is this the optimal next step?\n",
    "3. Could other actions be better right now?\n",
    "\n",
    "Score meaning:\n",
    "0: Invalid or counterproductive\n",
    "2: Valid but unhelpful\n",
    "4: Somewhat helps but inefficient\n",
    "6: Helpful but not optimal timing\n",
    "8: Very good next step\n",
    "10: Perfect action for current state\n",
    "\n",
    "Provide score (0-10):\"\"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback prompt for unparseable options\n",
    "            prompt = f\"\"\"Task: {query}\n",
    "Action: {option}\n",
    "Rate how this action helps the task (0-10):\"\"\"\n",
    "\n",
    "        gpt3_prompt_options.append(prompt)\n",
    "\n",
    "    # Process in batches\n",
    "    BATCH_SIZE = 20\n",
    "    scores = {}\n",
    "    for i in range(0, len(gpt3_prompt_options), BATCH_SIZE):\n",
    "        batch = gpt3_prompt_options[i:i + BATCH_SIZE]\n",
    "        batch_options = options[i:i + BATCH_SIZE]\n",
    "        \n",
    "        response = gpt3_call(\n",
    "            engine=engine,\n",
    "            prompt=batch,\n",
    "            max_tokens=5,\n",
    "            temperature=0.3)\n",
    "        \n",
    "        for option, choice in zip(batch_options, response[\"choices\"]):\n",
    "            try:\n",
    "                score_text = choice[\"text\"].strip()\n",
    "                score = float(next(s for s in score_text.split() if s.replace('.','',1).isdigit()))\n",
    "                scores[option] = score\n",
    "            except:\n",
    "                scores[option] = 0\n",
    "                \n",
    "        time.sleep(0.1)\n",
    "\n",
    "    return scores, response\n",
    "\n",
    "def make_options(pick_targets=None, place_targets=None, options_in_api_form=True, termination_string=\"done()\"):\n",
    "  if not pick_targets:\n",
    "    pick_targets = PICK_TARGETS\n",
    "  if not place_targets:\n",
    "    place_targets = PLACE_TARGETS\n",
    "  options = []\n",
    "  for pick in pick_targets:\n",
    "    for place in place_targets:\n",
    "      if options_in_api_form:\n",
    "        option = \"robot.pick_and_place({}, {})\".format(pick, place)\n",
    "      else:\n",
    "        option = \"Pick the {} and place it on the {}.\".format(pick, place)\n",
    "      options.append(option)\n",
    "\n",
    "  options.append(termination_string)\n",
    "  print(\"Considering\", len(options), \"options\")\n",
    "  return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Shp3CFFDzDie"
   },
   "outputs": [],
   "source": [
    "query = \"To pick the blue block and put it on the red block, I should:\\n\"\n",
    "options = make_options(PICK_TARGETS, PLACE_TARGETS)\n",
    "scores, response = gpt3_scoring(query, options, engine=ENGINE, limit_num_options=5, option_start='\\n', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Helper Functions\n",
    "Utility functions for:\n",
    "- Scene description\n",
    "- Score normalization\n",
    "- Visualization\n",
    "- Step conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "w16kuljCGd2o"
   },
   "outputs": [],
   "source": [
    "#@title Helper Functions\n",
    "\n",
    "def build_scene_description(found_objects, block_name=\"box\", bowl_name=\"circle\"):\n",
    "  scene_description = f\"objects = {found_objects}\"\n",
    "  scene_description = scene_description.replace(block_name, \"block\")\n",
    "  scene_description = scene_description.replace(bowl_name, \"bowl\")\n",
    "  scene_description = scene_description.replace(\"'\", \"\")\n",
    "  return scene_description\n",
    "\n",
    "def step_to_nlp(step):\n",
    "  step = step.replace(\"robot.pick_and_place(\", \"\")\n",
    "  step = step.replace(\")\", \"\")\n",
    "  pick, place = step.split(\", \")\n",
    "  return \"Pick the \" + pick + \" and place it on the \" + place + \".\"\n",
    "\n",
    "def normalize_scores(scores):\n",
    "  max_score = max(scores.values())  \n",
    "  normed_scores = {key: np.clip(scores[key] / max_score, 0, 1) for key in scores}\n",
    "  return normed_scores\n",
    "\n",
    "def plot_saycan(llm_scores, affordance_scores, combined_scores, step, show_top=10):\n",
    "    \"\"\"Enhanced visualization with numerical values\"\"\"\n",
    "    # Get top options\n",
    "    top_options = nlargest(show_top, combined_scores, key=combined_scores.get)\n",
    "    \n",
    "    # Format scores for display\n",
    "    scores_data = []\n",
    "    for opt in top_options:\n",
    "        scores_data.append({\n",
    "            'option': opt,\n",
    "            'llm': llm_scores[opt],\n",
    "            'affordance': affordance_scores[opt],\n",
    "            'combined': combined_scores[opt]\n",
    "        })\n",
    "    \n",
    "    # Print detailed scores\n",
    "    print(\"\\nDetailed Scores for Step:\")\n",
    "    for data in scores_data:\n",
    "        print(f\"\\nOption: {data['option']}\")\n",
    "        print(f\"LLM Score: {data['llm']:.3f}\")\n",
    "        print(f\"Affordance Score: {data['affordance']:.3f}\")\n",
    "        print(f\"Combined Score: {data['combined']:.3f}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    positions = np.arange(len(scores_data))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    plt.bar(positions - width, [d['affordance'] for d in scores_data], \n",
    "           width, label='Affordance', color='#ea9999ff')\n",
    "    plt.bar(positions, [d['llm'] for d in scores_data], \n",
    "           width, label='Language', color='#a4c2f4ff')\n",
    "    plt.bar(positions + width, [d['combined'] for d in scores_data], \n",
    "           width, label='Combined', color='#93CE8E')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i in positions:\n",
    "        plt.text(i - width, scores_data[i]['affordance'], \n",
    "                f\"{scores_data[i]['affordance']:.2f}\", ha='center', va='bottom')\n",
    "        plt.text(i, scores_data[i]['llm'], \n",
    "                f\"{scores_data[i]['llm']:.2f}\", ha='center', va='bottom')\n",
    "        plt.text(i + width, scores_data[i]['combined'], \n",
    "                f\"{scores_data[i]['combined']:.2f}\", ha='center', va='bottom')\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xticks(positions, [d['option'].replace('robot.pick_and_place(', '')\n",
    "                         .replace(')', '').replace(', ', '\\n') \n",
    "                         for d in scores_data], rotation=45)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f\"{step}\\nScore Components\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affordance Scoring\n",
    "Implements object-detection based affordance scoring without RL policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TyQ8GVvzyxP5"
   },
   "outputs": [],
   "source": [
    "def affordance_scoring(options, found_objects, verbose=False, block_name=\"box\", bowl_name=\"circle\", termination_string=\"done()\"):\n",
    "    \"\"\"Enhanced affordance scoring with physics-aware and task-relevant bonuses\"\"\"\n",
    "    affordance_scores = {}\n",
    "    \n",
    "    # Process found objects\n",
    "    found_objects = [\n",
    "        found_object.replace(block_name, \"block\").replace(bowl_name, \"bowl\")\n",
    "        for found_object in found_objects + list(PLACE_TARGETS.keys())[-5:]\n",
    "    ]\n",
    "    verbose and print(\"found_objects\", found_objects)\n",
    "    \n",
    "    # Get current object positions for physics-aware scoring\n",
    "    object_positions = {}\n",
    "    for obj_name, obj_id in env.obj_name_to_id.items():\n",
    "        if obj_id is not None:\n",
    "            pos, _ = pybullet.getBasePositionAndOrientation(obj_id)\n",
    "            object_positions[obj_name] = np.array(pos)\n",
    "    \n",
    "    for option in options:\n",
    "        if option == termination_string:\n",
    "            affordance_scores[option] = 0.2\n",
    "            continue\n",
    "            \n",
    "        # Parse option\n",
    "        pick, place = option.replace(\"robot.pick_and_place(\", \"\").replace(\")\", \"\").split(\", \")\n",
    "        base_score = 0\n",
    "        \n",
    "        # Check basic existence\n",
    "        found_objects_copy = found_objects.copy()\n",
    "        if pick in found_objects_copy:\n",
    "            found_objects_copy.remove(pick)\n",
    "            if place in found_objects_copy or place in PLACE_TARGETS:\n",
    "                base_score = 1.0\n",
    "        \n",
    "        if base_score == 0:\n",
    "            affordance_scores[option] = 0\n",
    "            continue\n",
    "            \n",
    "        # Apply physics-aware modifications\n",
    "        score_multiplier = 1.0\n",
    "        \n",
    "        # Check if pick object is accessible (not underneath something)\n",
    "        if pick in object_positions:\n",
    "            pick_pos = object_positions[pick]\n",
    "            for other_obj, other_pos in object_positions.items():\n",
    "                if other_obj != pick:\n",
    "                    # If another object is significantly above and close horizontally\n",
    "                    if (other_pos[2] > pick_pos[2] + 0.02 and \n",
    "                        np.linalg.norm(other_pos[:2] - pick_pos[:2]) < 0.05):\n",
    "                        score_multiplier *= 0.2  # Heavy penalty for blocked objects\n",
    "        \n",
    "        # Penalize stacking on unstable bases\n",
    "        if (\"block\" in place and place in object_positions and \n",
    "            any(pos[2] > object_positions[place][2] + 0.02 \n",
    "                for pos in object_positions.values())):\n",
    "            score_multiplier *= 0.5  # Penalty for stacking on already stacked blocks\n",
    "        \n",
    "        # Bonus for matching colors in color tasks\n",
    "        if \"matching\" in option.lower() or \"color\" in option.lower():\n",
    "            pick_color = pick.split()[0]\n",
    "            place_color = place.split()[0]\n",
    "            if pick_color == place_color:\n",
    "                score_multiplier *= 1.2\n",
    "        \n",
    "        # Bonus for corners in corner tasks\n",
    "        if \"corner\" in option.lower() and \"corner\" in place:\n",
    "            score_multiplier *= 1.1\n",
    "        \n",
    "        # Calculate final score\n",
    "        final_score = base_score * score_multiplier\n",
    "        affordance_scores[option] = np.clip(final_score, 0, 1)\n",
    "        \n",
    "        verbose and print(f\"{final_score:.2f} \\t {option}\")\n",
    "    \n",
    "    return affordance_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cell\n",
    "Demonstrates basic SayCan functionality with test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3xZm83muyRWR"
   },
   "outputs": [],
   "source": [
    "# #@title Test\n",
    "# termination_string = \"done()\"\n",
    "# query = \"To pick the blue block and put it on the red block, I should:\\n\"\n",
    "\n",
    "# options = make_options(PICK_TARGETS, PLACE_TARGETS, termination_string=termination_string)\n",
    "# llm_scores, _ = gpt3_scoring(query, options, verbose=True, engine=ENGINE)\n",
    "\n",
    "# affordance_scores = affordance_scoring(options, found_objects, block_name=\"box\", bowl_name=\"circle\", verbose=False, termination_string=termination_string)\n",
    "\n",
    "# combined_scores = {option: np.exp(llm_scores[option]) * affordance_scores[option] for option in options}\n",
    "# combined_scores = normalize_scores(combined_scores)\n",
    "# selected_task = max(combined_scores, key=combined_scores.get)\n",
    "# print(\"Selecting: \", selected_task)\n",
    "\n",
    "# # Added performance summary\n",
    "# print(\"\\nPerformance for test run:\")\n",
    "# performance_monitor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SayCan Demo (with affordance function)\n",
    "Final demo cells showing:\n",
    "1. Full SayCan implementation\n",
    "2. Socratic Model variant\n",
    "Each including:\n",
    "- Prompt setup\n",
    "- Task configuration\n",
    "- Scene setup\n",
    "- Execution with direct control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yys24Oj1796Y"
   },
   "outputs": [],
   "source": [
    "#@title Prompt\n",
    "\n",
    "termination_string = \"done()\"\n",
    "\n",
    "gpt3_context = \"\"\"\n",
    "objects = [red block, yellow block, blue block, green bowl]\n",
    "# move all the blocks to the top left corner.\n",
    "robot.pick_and_place(blue block, top left corner)\n",
    "robot.pick_and_place(red block, top left corner)\n",
    "robot.pick_and_place(yellow block, top left corner)\n",
    "done()\n",
    "\n",
    "objects = [red block, yellow block, blue block, green bowl]\n",
    "# put the yellow one the green thing.\n",
    "robot.pick_and_place(yellow block, green bowl)\n",
    "done()\n",
    "\n",
    "objects = [yellow block, blue block, red block]\n",
    "# move the light colored block to the middle.\n",
    "robot.pick_and_place(yellow block, middle)\n",
    "done()\n",
    "\n",
    "objects = [blue block, green bowl, red block, yellow bowl, green block]\n",
    "# stack the blocks.\n",
    "robot.pick_and_place(green block, blue block)\n",
    "robot.pick_and_place(red block, green block)\n",
    "done()\n",
    "\n",
    "objects = [red block, blue block, green bowl, blue bowl, yellow block, green block]\n",
    "# group the blue objects together.\n",
    "robot.pick_and_place(blue block, blue bowl)\n",
    "done()\n",
    "\n",
    "objects = [green bowl, red block, green block, red bowl, yellow bowl, yellow block]\n",
    "# sort all the blocks into their matching color bowls.\n",
    "robot.pick_and_place(green block, green bowl)\n",
    "robot.pick_and_place(red block, red bowl)\n",
    "robot.pick_and_place(yellow block, yellow bowl)\n",
    "done()\n",
    "\"\"\"\n",
    "\n",
    "use_environment_description = False\n",
    "gpt3_context_lines = gpt3_context.split(\"\\n\")\n",
    "gpt3_context_lines_keep = []\n",
    "for gpt3_context_line in gpt3_context_lines:\n",
    "  if \"objects =\" in gpt3_context_line and not use_environment_description:\n",
    "    continue\n",
    "  gpt3_context_lines_keep.append(gpt3_context_line)\n",
    "\n",
    "gpt3_context = \"\\n\".join(gpt3_context_lines_keep)\n",
    "print(gpt3_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "eCJckkuwFzDS"
   },
   "outputs": [],
   "source": [
    "#@title Task and Config\n",
    "# only_plan = False\n",
    "\n",
    "# raw_input = \"put all the blocks in different corners.\" \n",
    "# config = {\"pick\":  [\"red block\", \"yellow block\", \"green block\", \"blue block\"],\n",
    "#           \"place\": [\"red bowl\"]}\n",
    "\n",
    "# raw_input = \"move the block to the bowl.\"\n",
    "# config = {'pick':  ['red block'],\n",
    "#           'place': ['green bowl']}\n",
    "\n",
    "# raw_input = \"put any blocks on their matched colored bowls.\"\n",
    "# config = {'pick':  ['yellow block', 'green block', 'blue block'],\n",
    "#           'place': ['yellow bowl', 'green bowl', 'blue bowl']}\n",
    "          \n",
    "# raw_input = \"put all the blocks in the green bowl.\"\n",
    "# config = {'pick':  ['yellow block', 'green block', 'red block'],\n",
    "#           'place': ['yellow bowl', 'green bowl']}\n",
    "\n",
    "# raw_input = \"stack all the blocks.\"\n",
    "# config = {'pick':  ['yellow block', 'blue block', 'red block'],\n",
    "#           'place': ['blue bowl', 'red bowl']}\n",
    "\n",
    "# raw_input = \"make the highest block stack.\"\n",
    "# config = {'pick':  ['yellow block', 'blue block', 'red block'],\n",
    "#           'place': ['blue bowl', 'red bowl']}\n",
    "\n",
    "# raw_input = \"stack all the blocks.\"\n",
    "# config = {'pick':  ['green block', 'blue block', 'red block'],\n",
    "#           'place': ['yellow bowl', 'green bowl']}\n",
    "\n",
    "# raw_input = \"put the block in all the corners.\" \n",
    "# config = {'pick':  ['red block'],\n",
    "#           'place': ['red bowl', 'green bowl']}\n",
    "\n",
    "# raw_input = \"clockwise, move the block through all the corners.\"\n",
    "# config = {'pick':  ['red block'],\n",
    "#           'place': ['red bowl', 'green bowl', 'yellow bowl']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DX18LmbIIGfz"
   },
   "outputs": [],
   "source": [
    "#@title Setup Scene\n",
    "image_path = \"./2db.png\"\n",
    "np.random.seed(2)\n",
    "if config is None:\n",
    "  pick_items = list(PICK_TARGETS.keys())\n",
    "  pick_items = np.random.choice(pick_items, size=np.random.randint(1, 5), replace=False)\n",
    "\n",
    "  place_items = list(PLACE_TARGETS.keys())[:-9]\n",
    "  place_items = np.random.choice(place_items, size=np.random.randint(1, 6 - len(pick_items)), replace=False)\n",
    "  config = {\"pick\":  pick_items,\n",
    "            \"place\": place_items}\n",
    "  print(pick_items, place_items)\n",
    "\n",
    "# obs = env.reset(config)\n",
    "\n",
    "# img_top = env.get_camera_image_top()\n",
    "# img_top_rgb = cv2.cvtColor(img_top, cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(img_top)\n",
    "\n",
    "# imageio.imsave(image_path, img_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "m4Yh-PqZ-03n"
   },
   "outputs": [],
   "source": [
    "# #@title Runner\n",
    "# plot_on = True\n",
    "# max_tasks = 5\n",
    "\n",
    "# try:\n",
    "#     # Initialize scene and options\n",
    "#     options = make_options(PICK_TARGETS, PLACE_TARGETS, termination_string=termination_string)\n",
    "#     found_objects = vild(image_path, category_name_string, vild_params, plot_on=False)\n",
    "#     scene_description = build_scene_description(found_objects)\n",
    "#     env_description = scene_description\n",
    "#     print(scene_description)\n",
    "\n",
    "#     # Initialize prompts\n",
    "#     gpt3_prompt = gpt3_context\n",
    "#     if use_environment_description:\n",
    "#         gpt3_prompt += \"\\n\" + env_description\n",
    "#     gpt3_prompt += \"\\n# \" + raw_input + \"\\n\"\n",
    "\n",
    "#     # Initialize tracking lists\n",
    "#     all_llm_scores = []\n",
    "#     all_affordance_scores = []\n",
    "#     all_combined_scores = []\n",
    "#     num_tasks = 0\n",
    "#     selected_task = \"\"\n",
    "#     steps_text = []\n",
    "\n",
    "#     # Initialize executor if executing actions\n",
    "#     if not only_plan:\n",
    "#         executor = DirectExecutor(env)\n",
    "#         print('Initial state:')\n",
    "#         plt.imshow(env.get_camera_image())\n",
    "#         plt.show()\n",
    "\n",
    "#     while not selected_task == termination_string:\n",
    "#         num_tasks += 1\n",
    "#         if num_tasks > max_tasks:\n",
    "#             break\n",
    "\n",
    "#         # Update scene understanding and affordances\n",
    "#         found_objects = vild(image_path, category_name_string, vild_params, plot_on=False)\n",
    "#         affordance_scores = affordance_scoring(options, found_objects, block_name=\"box\",\n",
    "#                                             bowl_name=\"circle\", verbose=False)\n",
    "\n",
    "#         # Get language scores\n",
    "#         llm_scores, _ = gpt3_scoring(gpt3_prompt, options, verbose=True,\n",
    "#                                    engine=ENGINE, print_tokens=False)\n",
    "\n",
    "#         # Combine scores and select action\n",
    "#         combined_scores = {option: np.exp(llm_scores[option]) * affordance_scores[option]\n",
    "#                          for option in options}\n",
    "#         combined_scores = normalize_scores(combined_scores)\n",
    "#         selected_task = max(combined_scores, key=combined_scores.get)\n",
    "\n",
    "#         if selected_task and selected_task != termination_string:\n",
    "#             steps_text.append(selected_task)\n",
    "#             print(f\"{num_tasks} Selecting: {selected_task}\")\n",
    "#             gpt3_prompt += selected_task + \"\\n\"\n",
    "\n",
    "#             # Store scores for visualization\n",
    "#             all_llm_scores.append(llm_scores)\n",
    "#             all_affordance_scores.append(affordance_scores)\n",
    "#             all_combined_scores.append(combined_scores)\n",
    "\n",
    "#             # Execute action if not just planning\n",
    "#             if not only_plan:\n",
    "#                 nlp_step = step_to_nlp(selected_task)\n",
    "#                 print(f'Executing: {nlp_step}')\n",
    "#                 obs = executor.run(obs, nlp_step)\n",
    "#                 if obs is None:\n",
    "#                     print(\"Failed to execute step, stopping execution\")\n",
    "#                     break\n",
    "                \n",
    "#                 # Update image for next iteration\n",
    "#                 img_top = env.get_camera_image_top()\n",
    "#                 imageio.imwrite(image_path, img_top)\n",
    "\n",
    "#     # Visualization section\n",
    "#     if plot_on:\n",
    "#         for llm_scores, affordance_scores, combined_scores, step in zip(\n",
    "#                 all_llm_scores, all_affordance_scores, all_combined_scores, steps_text):\n",
    "#             plot_saycan(llm_scores, affordance_scores, combined_scores, step, show_top=10)\n",
    "\n",
    "#     # Print solution\n",
    "#     print('**** Solution ****')\n",
    "#     print(env_description)\n",
    "#     print('# ' + raw_input)\n",
    "#     for i, step in enumerate(steps_text):\n",
    "#         if step == '' or step == termination_string:\n",
    "#             break\n",
    "#         print('Step ' + str(i) + ': ' + step)\n",
    "\n",
    "#     # Show final state if executing\n",
    "#     if not only_plan:\n",
    "#         print('Final state:')\n",
    "#         plt.imshow(env.get_camera_image())\n",
    "#         plt.show()\n",
    "\n",
    "#         # Print performance metrics\n",
    "#         print(\"\\nOverall Performance Metrics:\")\n",
    "#         performance_monitor.print_summary()\n",
    "#         execution_tracker.print_summary()\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error in main execution: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWQwjuiDi1nC"
   },
   "source": [
    "### Socratic Model: VILD, GPT3, CLIPort Demo\n",
    "\n",
    "This implements a version of LLM planning shown in [Socratic Models](https://socraticmodels.github.io/), without the grounding, but with a scene description. For this relatively simple environment with clear robotic affordances, the scene description is generally sufficient. This mirrors the implementation attached to the paper [here](https://github.com/google-research/google-research/tree/master/socraticmodels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "trXP07uypWL8"
   },
   "outputs": [],
   "source": [
    "#@title Prompt\n",
    "\n",
    "gpt3_context = \"\"\"\n",
    "objects = [red block, yellow block, blue block, green bowl]\n",
    "# move all the blocks to the top left corner.\n",
    "robot.pick_and_place(blue block, top left corner)\n",
    "robot.pick_and_place(red block, top left corner)\n",
    "robot.pick_and_place(yellow block, top left corner)\n",
    "done()\n",
    "\n",
    "objects = [red block, yellow block, blue block, green bowl]\n",
    "# put the yellow one the green thing.\n",
    "robot.pick_and_place(yellow block, green bowl)\n",
    "done()\n",
    "\n",
    "objects = [yellow block, blue block, red block]\n",
    "# move the light colored block to the middle.\n",
    "robot.pick_and_place(yellow block, middle)\n",
    "done()\n",
    "\n",
    "objects = [blue block, green bowl, red block, yellow bowl, green block]\n",
    "# stack the blocks.\n",
    "robot.pick_and_place(green block, blue block)\n",
    "robot.pick_and_place(red block, green block)\n",
    "done()\n",
    "\n",
    "objects = [red block, blue block, green bowl, blue bowl, yellow block, green block]\n",
    "# group the blue objects together.\n",
    "robot.pick_and_place(blue block, blue bowl)\n",
    "done()\n",
    "\n",
    "objects = [green bowl, red block, green block, red bowl, yellow bowl, yellow block]\n",
    "# sort all the blocks into their matching color bowls.\n",
    "robot.pick_and_place(green block, green bowl)\n",
    "robot.pick_and_place(red block, red bowl)\n",
    "robot.pick_and_place(yellow block, yellow bowl)\n",
    "done()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "sPx--Ea7IZ69"
   },
   "outputs": [],
   "source": [
    "# #@title Queries and Configs\n",
    "\n",
    "# only_plan = False\n",
    "\n",
    "# raw_input = \"put all the blocks in different corners.\" \n",
    "# config = {'pick':  ['red block', 'yellow block', 'green block', 'blue block'],\n",
    "#           'place': ['red bowl']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "k6TSgRItpPC4"
   },
   "outputs": [],
   "source": [
    "# #@title Runner\n",
    "\n",
    "# env_description = ''\n",
    "# image_path = './2db.png'\n",
    "\n",
    "# np.random.seed(2)\n",
    "\n",
    "# if config is None:\n",
    "#   pick_items = list(PICK_TARGETS.keys())\n",
    "#   pick_items = np.random.choice(pick_items, size=np.random.randint(1, 5), replace=False)\n",
    "\n",
    "#   place_items = list(PLACE_TARGETS.keys())[:-9]\n",
    "#   place_items = np.random.choice(place_items, size=np.random.randint(1, 6 - len(pick_items)), replace=False)\n",
    "#   config = {'pick':  pick_items,\n",
    "#             'place': place_items}\n",
    "#   print(pick_items, place_items)\n",
    "# obs = env.reset(config)\n",
    "\n",
    "# img_top = env.get_camera_image_top()\n",
    "# img_top_rgb = cv2.cvtColor(img_top, cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(img_top_rgb)\n",
    "\n",
    "# imageio.imsave(image_path, img_top)\n",
    "\n",
    "# found_objects = vild(image_path, category_name_string, vild_params, plot_on=False)\n",
    "# scene_description = build_scene_description(found_objects)\n",
    "# print(scene_description)\n",
    "\n",
    "# env_description = scene_description\n",
    "\n",
    "# gpt3_prompt = gpt3_context\n",
    "# gpt3_prompt += \"\\n\" + env_description + \"\\n\"\n",
    "# gpt3_prompt += \"# \" + raw_input\n",
    "# response = gpt3_call(engine=ENGINE, prompt=gpt3_prompt, max_tokens=128, temperature=0)\n",
    "# steps_text = [text.strip().strip() for text in response[\"choices\"][0][\"text\"].strip().split(\"#\")[0].split(\"\\n\")][:-1]\n",
    "# print('**** Solution ****')\n",
    "# print(env_description)\n",
    "# print('# ' + raw_input)\n",
    "# for i, step in enumerate(steps_text):\n",
    "#   if step == '' or step == termination_string:\n",
    "#     break\n",
    "#   print('Step ' + str(i) + ': ' + step)\n",
    "#   nlp_step = step_to_nlp(step)\n",
    "\n",
    "# if not only_plan:\n",
    "#   print('Initial state:')\n",
    "#   plt.imshow(env.get_camera_image())\n",
    "\n",
    "#   # Initialize executor\n",
    "#   executor = DirectExecutor(env)\n",
    "\n",
    "#   for i, step in enumerate(steps_text):\n",
    "#     if step == '' or step == termination_string:\n",
    "#       break\n",
    "#     nlp_step = step_to_nlp(step)\n",
    "#     print('GPT-3 says next step:', nlp_step)\n",
    "#     obs = executor.run(obs, nlp_step)\n",
    "\n",
    "#   # Show camera image after task.\n",
    "#   print('Final state:')\n",
    "#   plt.imshow(env.get_camera_image())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Josh's SayCan Demo Cell\n",
    "\n",
    "### Overview\n",
    "This demonstration shows the complete SayCan pipeline converting natural language instructions into robot actions through LLM planning and affordance scoring.\n",
    "\n",
    "### Pipeline Steps\n",
    "1. **Vision Analysis**\n",
    "   * Scene detection using ViLD\n",
    "   * Object identification and location mapping\n",
    "\n",
    "2. **Language Planning**\n",
    "   * GPT-3 converts high-level instruction to steps\n",
    "   * Each possible action gets scored for relevance\n",
    "\n",
    "3. **Action Selection**\n",
    "   * Combines language scores with physical affordances \n",
    "   * Selects optimal action at each step\n",
    "\n",
    "4. **Execution**\n",
    "   * Runs selected action through direct control \n",
    "   * Shows visual confirmation of results\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "# Simple Instructions\n",
    "instruction = \"stack the red block on the blue block\"\n",
    "run_saycan_demo_v2(instruction)\n",
    "\n",
    "# Complex Instructions\n",
    "instruction = \"put all blocks in their matching colored bowls\"\n",
    "run_saycan_demo_v2(instruction)\n",
    "```\n",
    "\n",
    "### Visualization Features\n",
    "* Initial scene inspection\n",
    "* Action scoring visualization\n",
    "* Before/after images per step\n",
    "* Performance metrics display\n",
    "\n",
    "### Try These Instructions\n",
    "* `\"pick up the red block\"`\n",
    "* `\"put the blue block in the green bowl\"`\n",
    "* `\"sort blocks into matching colored bowls\"`\n",
    "* `\"put blocks in different corners\"`\n",
    "\n",
    "### Notes\n",
    "* Each step shows both scoring and execution\n",
    "* Visual confirmation helps track progress\n",
    "* Performance metrics show timing and success rates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title Josh's SayCan Demo - With Scoring\n",
    "# def run_saycan_with_scoring(instruction):\n",
    "#     \"\"\"SayCan with explicit scoring of each possible action\"\"\"\n",
    "#     print(f\" Running SayCan for: {instruction}\")\n",
    "    \n",
    "#     # Reset environment\n",
    "#     config = {\n",
    "#         'pick': ['red block', 'blue block', 'green block'],\n",
    "#         'place': ['red bowl', 'blue bowl', 'green bowl']\n",
    "#     }\n",
    "#     obs = env.reset(config)\n",
    "\n",
    "#     executor = DirectExecutor(env)\n",
    "    \n",
    "#     # Show initial scene\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.imshow(env.get_camera_image())\n",
    "#     plt.title(\"Initial Scene\")\n",
    "#     plt.show()\n",
    "\n",
    "#     # Get scene description\n",
    "#     image_path = 'tmp.jpg'\n",
    "#     img_top = env.get_camera_image_top()\n",
    "#     imageio.imwrite(image_path, img_top)\n",
    "#     found_objects = vild(image_path, category_name_string, vild_params, plot_on=False)\n",
    "#     scene_description = build_scene_description(found_objects)\n",
    "#     print(\"\\n Scene Description:\", scene_description)\n",
    "\n",
    "#     # Generate and execute plan with scoring\n",
    "#     options = make_options(PICK_TARGETS, PLACE_TARGETS, termination_string=\"done()\")\n",
    "#     gpt3_prompt = gpt3_context + \"\\n\" + scene_description + \"\\n# \" + instruction + \"\\n\"\n",
    "    \n",
    "#     num_tasks = 0\n",
    "#     selected_task = \"\"\n",
    "#     while not selected_task == \"done()\":\n",
    "#         num_tasks += 1\n",
    "#         if num_tasks > 5:  # Limit number of steps\n",
    "#             break\n",
    "            \n",
    "#         # Score all possible actions\n",
    "#         print(f\"\\n Step {num_tasks}:\")\n",
    "#         llm_scores, _ = gpt3_scoring(gpt3_prompt, options, verbose=False, engine=ENGINE)\n",
    "#         affordance_scores = affordance_scoring(options, found_objects)\n",
    "        \n",
    "#         # Combine scores and select best action\n",
    "#         combined_scores = {option: np.exp(llm_scores[option]) * affordance_scores[option] \n",
    "#                          for option in options}\n",
    "#         combined_scores = normalize_scores(combined_scores)\n",
    "#         selected_task = max(combined_scores, key=combined_scores.get)\n",
    "\n",
    "        \n",
    "        \n",
    "#         if selected_task and selected_task != \"done()\":\n",
    "#             print(f\"Selected action: {selected_task}\")\n",
    "            \n",
    "#             # Plot scores for this step\n",
    "#             plot_saycan(llm_scores, affordance_scores, combined_scores, \n",
    "#                        f\"Step {num_tasks} Scores\", show_top=5)\n",
    "            \n",
    "#             # Execute action\n",
    "#             nlp_step = step_to_nlp(selected_task)\n",
    "#             print(f\"Executing: {nlp_step}\")\n",
    "#             obs = executor.run(obs, nlp_step)\n",
    "#             gpt3_prompt += selected_task + \"\\n\"\n",
    "            \n",
    "#     # Show final scene\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.imshow(env.get_camera_image())\n",
    "#     plt.title(\"Final Scene\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: SayCan vs Socratic Comparison\n",
    "\n",
    "This experiment compares the performance of SayCan and Socratic approaches across several key tasks:\n",
    "1. Spatial reasoning (corners placement)\n",
    "2. Color matching\n",
    "3. Stacking\n",
    "4. Complex multi-step tasks\n",
    "\n",
    "We measure:\n",
    "- Success rates\n",
    "- Number of steps taken\n",
    "- Execution time\n",
    "- LLM scoring patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import traceback \n",
    "\n",
    "\n",
    "def setup_gpt3_context(use_environment_description=False):\n",
    "    \"\"\"Setup and process GPT-3 context following SayCan demo approach\"\"\"\n",
    "    \n",
    "    experiment_gpt3_context = \"\"\"\n",
    "    # Rules for robot actions:\n",
    "    - Blocks can only be placed on valid targets\n",
    "    - Each corner can hold one block\n",
    "    - Corners are: top left, top right, bottom left, bottom right\n",
    "    \n",
    "    objects = [red block, blue block, green block]\n",
    "    # put blocks in different corners\n",
    "    robot.pick_and_place(red block, top left corner)\n",
    "    robot.pick_and_place(blue block, top right corner)\n",
    "    robot.pick_and_place(green block, bottom left corner)\n",
    "    done()\n",
    "    \n",
    "    objects = [red block, blue block, green block]\n",
    "    # stack all blocks\n",
    "    robot.pick_and_place(red block, middle)\n",
    "    robot.pick_and_place(blue block, red block)\n",
    "    robot.pick_and_place(green block, blue block)\n",
    "    done()\n",
    "    \n",
    "    objects = [red block, blue block, green block, red bowl, blue bowl, green bowl]\n",
    "    # sort blocks into matching bowls\n",
    "    robot.pick_and_place(red block, red bowl)\n",
    "    robot.pick_and_place(blue block, blue bowl)\n",
    "    robot.pick_and_place(green block, green bowl)\n",
    "    done()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process context the same way as demo\n",
    "    context_lines = experiment_gpt3_context.split(\"\\n\")\n",
    "    context_lines_keep = []\n",
    "    for line in context_lines:\n",
    "        if \"objects =\" in line and not use_environment_description:\n",
    "            continue\n",
    "        context_lines_keep.append(line)\n",
    "    \n",
    "    return \"\\n\".join(context_lines_keep)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskResult:\n",
    "    \"\"\"Stores results for a single task execution\"\"\"\n",
    "    method: str  # 'saycan' or 'socratic'\n",
    "    task: str \n",
    "    success: bool\n",
    "    num_steps: int\n",
    "    planned_steps: List[str]\n",
    "    executed_steps: List[str]\n",
    "    execution_times: List[float]\n",
    "    llm_scores: List[Dict[str, float]]\n",
    "    total_time: float\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"Handles running comparative experiments between SayCan and Socratic approaches\"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, env, model=\"gpt-3.5-turbo-instruct\"):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.results: List[TaskResult] = []\n",
    "        self.executor = DirectExecutor(env)\n",
    "        self.current_task = None  # Store current task for affordance scoring context\n",
    "        self.current_task_scores = {\n",
    "            'llm': [],\n",
    "            'affordance': [],\n",
    "            'combined': [],\n",
    "            'steps': []\n",
    "        }\n",
    "        self.gpt3_context = setup_gpt3_context(use_environment_description=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def get_enhanced_affordance_score(self, option: str, found_objects: list) -> float:\n",
    "        \"\"\"Enhanced affordance scoring with spatial awareness\"\"\"\n",
    "        if option == \"done()\":\n",
    "            return 0.2\n",
    "            \n",
    "        try:\n",
    "            pick, place = option.replace(\"robot.pick_and_place(\", \"\").replace(\")\", \"\").split(\", \")\n",
    "        except:\n",
    "            return 0.0\n",
    "        \n",
    "        # Base validity checks\n",
    "        if pick == place or pick not in found_objects:\n",
    "            return 0.0\n",
    "        if place not in found_objects and place not in PLACE_TARGETS:\n",
    "            return 0.0\n",
    "            \n",
    "        score = 1.0\n",
    "        \n",
    "        # Task-specific scoring\n",
    "        if self.current_task and \"corner\" in self.current_task.lower():\n",
    "            # Penalize non-corner targets\n",
    "            if \"corner\" not in place:\n",
    "                score *= 0.2\n",
    "                \n",
    "            # Check if corner is already occupied\n",
    "            if place in PLACE_TARGETS:  # If it's a corner\n",
    "                target_pos = PLACE_TARGETS[place]\n",
    "                if target_pos is not None:  # Make sure we have valid coordinates\n",
    "                    target_pos = np.array(target_pos)\n",
    "                    for obj_name, obj_id in self.env.obj_name_to_id.items():\n",
    "                        if obj_id is not None:\n",
    "                            obj_pos = np.array(pybullet.getBasePositionAndOrientation(obj_id)[0])\n",
    "                            # If any object is close to this corner\n",
    "                            if np.linalg.norm(obj_pos[:2] - target_pos[:2]) < 0.1:\n",
    "                                score *= 0.1  # Heavily penalize already occupied corners\n",
    "        \n",
    "        # Accessibility check\n",
    "        if pick in self.env.obj_name_to_id:\n",
    "            pick_id = self.env.obj_name_to_id[pick]\n",
    "            if pick_id is not None:\n",
    "                pick_pos = np.array(pybullet.getBasePositionAndOrientation(pick_id)[0])\n",
    "                for obj_name, obj_id in self.env.obj_name_to_id.items():\n",
    "                    if obj_id is not None and obj_name != pick:\n",
    "                        obj_pos = np.array(pybullet.getBasePositionAndOrientation(obj_id)[0])\n",
    "                        if (obj_pos[2] > pick_pos[2] + 0.02 and \n",
    "                            np.linalg.norm(obj_pos[:2] - pick_pos[:2]) < 0.05):\n",
    "                            score *= 0.1  # Severe penalty for blocked objects\n",
    "        \n",
    "        return np.clip(score, 0, 1)\n",
    "\n",
    "    def correct_scene_description(self, found_objects):\n",
    "        \"\"\"Correct color detection issues in scene description\"\"\"\n",
    "        corrections = {\n",
    "            'yellow bowl': 'red bowl',\n",
    "            'yellow block': 'red block'\n",
    "        }\n",
    "        corrected_objects = []\n",
    "        for obj in found_objects:\n",
    "            obj = obj.strip()\n",
    "            if obj in corrections:\n",
    "                corrected_objects.append(corrections[obj])\n",
    "            else:\n",
    "                corrected_objects.append(obj)\n",
    "        return corrected_objects\n",
    "\n",
    "    def clean_step(self, step: str) -> str:\n",
    "        \"\"\"Clean and validate a step string\"\"\"\n",
    "        step = step.strip()\n",
    "        if step.startswith('.'):\n",
    "            step = step.lstrip('.')\n",
    "        step = step.strip()\n",
    "        if ',' not in step or 'robot.pick_and_place' not in step:\n",
    "            return \"\"\n",
    "        return step\n",
    "        \n",
    "    def print_task_summary(self, task: str, method: str, planned_steps: List[str], executed_steps: List[str]):\n",
    "        \"\"\"Print detailed summary of task execution with clear method identification\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"TASK EXECUTION SUMMARY - {method.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Task: {task}\")\n",
    "        print(\"\\nPlanned Steps:\")\n",
    "        for i, step in enumerate(planned_steps, 1):\n",
    "            print(f\"{i}. {step}\")\n",
    "        print(\"\\nExecuted Steps:\")\n",
    "        for i, step in enumerate(executed_steps, 1):\n",
    "            print(f\"{i}. {step}\")\n",
    "            \n",
    "    def visualize_step_scores(self, step_num: int):\n",
    "        \"\"\"Visualize scores for a single step\"\"\"\n",
    "        if not self.current_task_scores['steps']:\n",
    "            return\n",
    "            \n",
    "        idx = step_num - 1\n",
    "        if idx >= len(self.current_task_scores['steps']):\n",
    "            return\n",
    "            \n",
    "        plot_saycan(\n",
    "            self.current_task_scores['llm'][idx],\n",
    "            self.current_task_scores['affordance'][idx],\n",
    "            self.current_task_scores['combined'][idx],\n",
    "            f\"Step {step_num}: {self.current_task_scores['steps'][idx]}\",\n",
    "            show_top=10\n",
    "        )\n",
    "        \n",
    "    def run_single_task(self, task: str, method: str, config: Dict) -> TaskResult:\n",
    "        \"\"\"Run a single task with specified method and collect metrics\"\"\"\n",
    "        self.current_task = task  # Store for affordance context\n",
    "        self.current_task_scores = {\n",
    "            'llm': [],\n",
    "            'affordance': [],\n",
    "            'combined': [],\n",
    "            'steps': []\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Reset environment without showing\n",
    "            obs = self.env.reset(config)\n",
    "            \n",
    "            # Initialize tracking\n",
    "            planned_steps = []\n",
    "            executed_steps = []\n",
    "            execution_times = []\n",
    "            llm_scores = []\n",
    "            \n",
    "            # Print task header\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Task: {task}\")\n",
    "            print(f\"Method: {method.upper()}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "            \n",
    "            if method == \"saycan\":\n",
    "                # Run SayCan implementation\n",
    "                options = make_options(PICK_TARGETS, PLACE_TARGETS)\n",
    "                found_objects = vild(\"tmp.jpg\", category_name_string, vild_params, plot_on=False)\n",
    "                found_objects = self.correct_scene_description(found_objects)\n",
    "                scene_description = build_scene_description(found_objects)\n",
    "                print(\"Scene:\", scene_description)\n",
    "                print(\"\\nStarting execution:\")\n",
    "                self.show_execution_state()\n",
    "\n",
    "                gpt3_prompt = self.gpt3_context\n",
    "                if use_environment_description:\n",
    "                    gpt3_prompt += \"\\n\" + scene_description\n",
    "                gpt3_prompt += \"\\n# \" + task + \"\\n\"\n",
    "\n",
    "                # Track steps\n",
    "                num_steps = 0\n",
    "                selected_task = \"\"\n",
    "                prev_selected_tasks = set()\n",
    "\n",
    "                while selected_task != \"done()\" and num_steps < 5:\n",
    "                    step_start = time.time()\n",
    "                    num_steps += 1\n",
    "\n",
    "                    # Get scores with enhanced affordance\n",
    "                    llm_scores_step, _ = gpt3_scoring(gpt3_prompt, options, engine=self.model, verbose=False)\n",
    "                    affordance_scores = {opt: self.get_enhanced_affordance_score(opt, found_objects) \n",
    "                                    for opt in options}\n",
    "\n",
    "                    # Print detailed scoring breakdown\n",
    "                    print(f\"\\nScoring Breakdown for Step {num_steps}:\")\n",
    "                    for option in options:\n",
    "                        if option == \"done()\":\n",
    "                            continue\n",
    "                        pick, place = option.replace(\"robot.pick_and_place(\", \"\").replace(\")\", \"\").split(\", \")\n",
    "                        llm_score = llm_scores_step[option]\n",
    "                        affordance_score = affordance_scores[option]\n",
    "                        combined = np.exp(llm_score) * affordance_score\n",
    "                        \n",
    "                        print(f\"\\nOption: {option}\")\n",
    "                        print(f\"  LLM Score: {llm_score:.3f} - {'Valid for task' if llm_score > 5 else 'Less relevant for task'}\")\n",
    "                        print(f\"  Affordance Score: {affordance_score:.3f} - Factors:\")\n",
    "                        print(f\"    - Object exists: {1 if pick in found_objects else 0}\")\n",
    "                        print(f\"    - Target valid: {1 if place in PLACE_TARGETS or place in found_objects else 0}\")\n",
    "                        if \"corner\" in task.lower():\n",
    "                            print(f\"    - Corner target: {'Yes' if 'corner' in place else 'No'}\")\n",
    "                        print(f\"  Combined Score: {combined:.3f}\")\n",
    "\n",
    "                    # Store scores for visualization\n",
    "                    combined_scores = {opt: np.exp(llm_scores_step[opt]) * affordance_scores[opt] \n",
    "                                    for opt in options}\n",
    "                    combined_scores = normalize_scores(combined_scores)\n",
    "                    \n",
    "                    self.current_task_scores['llm'].append(llm_scores_step)\n",
    "                    self.current_task_scores['affordance'].append(affordance_scores)\n",
    "                    self.current_task_scores['combined'].append(combined_scores)\n",
    "\n",
    "                    # Select best non-repeated action\n",
    "                    valid_options = [opt for opt in combined_scores.keys() \n",
    "                                    if opt not in prev_selected_tasks and opt != \"done()\"\n",
    "                                    and affordance_scores[opt] > 0]  # Only consider physically possible actions\n",
    "                    if not valid_options:\n",
    "                        selected_task = \"done()\"\n",
    "                        continue\n",
    "                        \n",
    "                    selected_task = max(valid_options, key=lambda x: combined_scores[x])\n",
    "                    \n",
    "                    if selected_task and selected_task != \"done()\":\n",
    "                        print(f\"\\nStep {num_steps}: {selected_task}\")\n",
    "                        planned_steps.append(selected_task)\n",
    "                        self.current_task_scores['steps'].append(selected_task)\n",
    "                        prev_selected_tasks.add(selected_task)\n",
    "\n",
    "                        # Visualize scores for this step\n",
    "                        self.visualize_step_scores(num_steps)\n",
    "\n",
    "                        # Execute\n",
    "                        nlp_step = step_to_nlp(selected_task)\n",
    "                        print(f\"Executing: {nlp_step}\")\n",
    "                        obs = self.executor.execute_action(obs, nlp_step)\n",
    "                        executed_steps.append(nlp_step)\n",
    "                        self.show_execution_state()\n",
    "\n",
    "                        step_time = time.time() - step_start\n",
    "                        execution_times.append(step_time)\n",
    "                        gpt3_prompt += selected_task + \"\\n\"\n",
    "\n",
    "                # Print final summary\n",
    "                self.print_task_summary(task, method, planned_steps, executed_steps)\n",
    "                \n",
    "            else:  # Socratic method\n",
    "                found_objects = vild(\"tmp.jpg\", category_name_string, vild_params, plot_on=False)\n",
    "                found_objects = self.correct_scene_description(found_objects)\n",
    "                scene_description = build_scene_description(found_objects)\n",
    "                print(\"Scene:\", scene_description)\n",
    "                \n",
    "                # Generate full plan\n",
    "                gpt3_prompt = gpt3_context + f\"\\n{scene_description}\\n# {task}\"\n",
    "                response = gpt3_call(engine=self.model, prompt=gpt3_prompt, max_tokens=128)\n",
    "                steps = [s.strip() for s in response[\"choices\"][0][\"text\"].strip().split(\"\\n\")]\n",
    "                \n",
    "                # Clean and validate steps\n",
    "                planned_steps = []\n",
    "                for step in steps:\n",
    "                    cleaned_step = self.clean_step(step)\n",
    "                    if cleaned_step:\n",
    "                        planned_steps.append(cleaned_step)\n",
    "                \n",
    "                print(\"\\nGenerated plan:\")\n",
    "                for i, step in enumerate(planned_steps, 1):\n",
    "                    print(f\"{i}. {step}\")\n",
    "                \n",
    "                print(\"\\nStarting execution:\")\n",
    "                self.show_execution_state()\n",
    "                \n",
    "                # Execute plan\n",
    "                for i, step in enumerate(planned_steps, 1):\n",
    "                    step_start = time.time()\n",
    "                    nlp_step = step_to_nlp(step)\n",
    "                    print(f\"\\nExecuting step {i}: {nlp_step}\")\n",
    "                    obs = self.executor.execute_action(obs, nlp_step)\n",
    "                    if obs is None:\n",
    "                        print(f\"Failed to execute step {i}\")\n",
    "                        break\n",
    "                    executed_steps.append(nlp_step)\n",
    "                    self.show_execution_state()\n",
    "                    execution_times.append(time.time() - step_start)\n",
    "                \n",
    "                # Print final summary\n",
    "                self.print_task_summary(task, method, planned_steps, executed_steps)\n",
    "            \n",
    "            success = (len(executed_steps) == len(planned_steps) and \n",
    "                      obs is not None and \n",
    "                      len(executed_steps) > 0)\n",
    "            print(f\"\\nTask completed. Success: {success}\")\n",
    "            \n",
    "            return TaskResult(\n",
    "                method=method,\n",
    "                task=task,\n",
    "                success=success,\n",
    "                num_steps=len(executed_steps),\n",
    "                planned_steps=planned_steps,\n",
    "                executed_steps=executed_steps,\n",
    "                execution_times=execution_times,\n",
    "                llm_scores=llm_scores,\n",
    "                total_time=time.time() - start_time\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during execution: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return TaskResult(\n",
    "                method=method,\n",
    "                task=task,\n",
    "                success=False,\n",
    "                num_steps=0,\n",
    "                planned_steps=[],\n",
    "                executed_steps=[],\n",
    "                execution_times=[],\n",
    "                llm_scores=[],\n",
    "                total_time=time.time() - start_time,\n",
    "                error=str(e)\n",
    "            )\n",
    "\n",
    "    def show_execution_state(self):\n",
    "        \"\"\"Show current environment state\"\"\"\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(self.env.get_camera_image())\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    def run_experiment(self, tasks: List[Dict], random_seed: int = 42) -> Dict[str, Any]:\n",
    "        \"\"\"Run full experiment comparing both methods across multiple tasks\"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        results = defaultdict(list)\n",
    "        for task_config in tasks:\n",
    "            for method in ['saycan', 'socratic']:\n",
    "                result = self.run_single_task(\n",
    "                    task=task_config['instruction'],\n",
    "                    method=method,\n",
    "                    config=task_config['config']\n",
    "                )\n",
    "                self.results.append(result)\n",
    "                results[method].append(result)\n",
    "                \n",
    "        # Calculate summary metrics\n",
    "        summary = {\n",
    "            'saycan': {\n",
    "                'success_rate': np.mean([r.success for r in results['saycan']]),\n",
    "                'avg_steps': np.mean([r.num_steps for r in results['saycan']]),\n",
    "                'avg_time': np.mean([r.total_time for r in results['saycan']])\n",
    "            },\n",
    "            'socratic': {\n",
    "                'success_rate': np.mean([r.success for r in results['socratic']]),\n",
    "                'avg_steps': np.mean([r.num_steps for r in results['socratic']]),\n",
    "                'avg_time': np.mean([r.total_time for r in results['socratic']])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create visualization of experiment results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to visualize\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Success rates\n",
    "        plt.subplot(131)\n",
    "        success_rates = {\n",
    "            'SayCan': np.mean([r.success for r in self.results if r.method == 'saycan']),\n",
    "            'Socratic': np.mean([r.success for r in self.results if r.method == 'socratic'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test tasks\n",
    "test_tasks = [\n",
    "    {\n",
    "        'instruction': 'put all the blocks in different corners',\n",
    "        'config': {\n",
    "            'pick': ['red block', 'blue block', 'green block'],\n",
    "            'place': ['red bowl', 'blue bowl', 'green bowl']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'put blocks in their matching colored bowls',\n",
    "        'config': {\n",
    "            'pick': ['red block', 'blue block', 'green block'],\n",
    "            'place': ['red bowl', 'blue bowl', 'green bowl']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'stack all the blocks',\n",
    "        'config': {\n",
    "            'pick': ['red block', 'blue block', 'green block'],\n",
    "            'place': ['red bowl', 'blue bowl', 'green bowl']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'put the red block between the blue and green blocks',\n",
    "        'config': {\n",
    "            'pick': ['red block', 'blue block', 'green block'],\n",
    "            'place': ['red bowl', 'blue bowl', 'green bowl']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run experiment\n",
    "# experiment = ExperimentRunner(env, model=\"gpt-3.5-turbo-instruct\")\n",
    "# summary = experiment.run_experiment(test_tasks)\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nExperiment Summary:\")\n",
    "# print(\"\\nSayCan Results:\")\n",
    "# for metric, value in summary['saycan'].items():\n",
    "#     print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "# print(\"\\nSocratic Results:\")\n",
    "# for metric, value in summary['socratic'].items():\n",
    "#     print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "# # Visualize results\n",
    "# experiment.visualize_results()\n",
    "\n",
    "# # Print detailed task breakdown\n",
    "# print(\"\\nDetailed Task Results:\")\n",
    "# for result in experiment.results:\n",
    "#     print(f\"\\nMethod: {result.method}\")\n",
    "#     print(f\"Task: {result.task}\")\n",
    "#     print(f\"Success: {result.success}\")\n",
    "#     print(f\"Steps Planned: {len(result.planned_steps)}\")\n",
    "#     print(f\"Steps Executed: {len(result.executed_steps)}\")\n",
    "#     print(f\"Total Time: {result.total_time:.2f}s\")\n",
    "#     if result.error:\n",
    "#         print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Pipeline Performance & Optimization \n",
    "\n",
    "This experiment analyzes how different configurations affect SayCan's performance and efficiency:\n",
    "\n",
    "### 1. Processing & Memory\n",
    "   - Processing in batches vs one at a time\n",
    "   - Checking memory usage \n",
    "   - Looking at API costs\n",
    "\n",
    "### 2. Model Behavior\n",
    "   - How fast different parts work\n",
    "   - Response times from GPT model\n",
    "   - If caching helps speed things up\n",
    "\n",
    "### 3. Computer Resources\n",
    "   - Running multiple things at once\n",
    "   - Where things slow down\n",
    "   - How GPU and CPU work together\n",
    "\n",
    "### We measure:\n",
    "- How long actions take\n",
    "- How many actions we can do per second \n",
    "- How much it costs to run\n",
    "- How much memory it uses\n",
    "- If saved results help speed things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for a pipeline run\"\"\"\n",
    "    batch_size: int\n",
    "    model_name: str\n",
    "    use_caching: bool\n",
    "    max_concurrent: int\n",
    "\n",
    "class PipelineOptimizer:\n",
    "    def __init__(self, env, base_config: Dict = None):\n",
    "        self.env = env\n",
    "        self.base_config = base_config or {\n",
    "            'pick': ['red block', 'blue block', 'green block'],\n",
    "            'place': ['red bowl', 'blue bowl', 'green bowl']\n",
    "        }\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.latencies = []\n",
    "        self.throughputs = []\n",
    "        self.memory_usage = []\n",
    "        self.api_costs = []\n",
    "        \n",
    "    def run_pipeline_benchmark(self, config: PipelineConfig, num_trials: int = 5):\n",
    "        \"\"\"Run benchmark with given configuration\"\"\"\n",
    "        metrics = {\n",
    "            'latency': [],\n",
    "            'throughput': [],\n",
    "            'memory': [],\n",
    "            'cost': []\n",
    "        }\n",
    "        \n",
    "        for _ in range(num_trials):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Reset environment\n",
    "            obs = self.env.reset(self.base_config)\n",
    "            \n",
    "            # Run standard test task\n",
    "            instruction = \"put all blocks in their matching colored bowls\"\n",
    "            \n",
    "            # Configure batch processing\n",
    "            options = make_options(PICK_TARGETS, PLACE_TARGETS)\n",
    "            options = [options[i:i + config.batch_size] for i in range(0, len(options), config.batch_size)]\n",
    "            \n",
    "            # Track API usage\n",
    "            initial_cost = performance_monitor.total_cost\n",
    "            \n",
    "            try:\n",
    "                # Run pipeline stages\n",
    "                for batch in options:\n",
    "                    # Vision processing\n",
    "                    found_objects = vild(\"tmp.jpg\", category_name_string, vild_params, plot_on=False)\n",
    "                    \n",
    "                    # Language model scoring\n",
    "                    llm_scores, _ = gpt3_scoring(instruction, batch, engine=config.model_name)\n",
    "                    \n",
    "                    # Affordance scoring\n",
    "                    affordance_scores = affordance_scoring(batch, found_objects)\n",
    "                    \n",
    "                    # Action selection\n",
    "                    combined_scores = {opt: np.exp(llm_scores[opt]) * affordance_scores[opt] \n",
    "                                    for opt in batch}\n",
    "                \n",
    "                total_time = time.time() - start_time\n",
    "                metrics['latency'].append(total_time)\n",
    "                metrics['throughput'].append(len(options) / total_time)\n",
    "                metrics['cost'].append(performance_monitor.total_cost - initial_cost)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in pipeline run: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return {k: np.mean(v) for k, v in metrics.items()}\n",
    "    \n",
    "    def optimize_pipeline(self, param_grid: Dict[str, List]):\n",
    "        \"\"\"Run grid search over pipeline parameters\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Generate configurations\n",
    "        configs = []\n",
    "        for batch_size in param_grid['batch_sizes']:\n",
    "            for model in param_grid['models']:\n",
    "                for cache in param_grid['use_cache']:\n",
    "                    for concurrent in param_grid['max_concurrent']:\n",
    "                        configs.append(PipelineConfig(\n",
    "                            batch_size=batch_size,\n",
    "                            model_name=model,\n",
    "                            use_caching=cache,\n",
    "                            max_concurrent=concurrent\n",
    "                        ))\n",
    "        \n",
    "        # Run benchmarks\n",
    "        for config in configs:\n",
    "            metrics = self.run_pipeline_benchmark(config)\n",
    "            results.append((config, metrics))\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, results: List[Tuple[PipelineConfig, Dict]]):\n",
    "        \"\"\"Visualize optimization results\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Extract data\n",
    "        batch_sizes = [r[0].batch_size for r in results]\n",
    "        latencies = [r[1]['latency'] for r in results]\n",
    "        throughputs = [r[1]['throughput'] for r in results]\n",
    "        costs = [r[1]['cost'] for r in results]\n",
    "        \n",
    "        # Latency vs Batch Size\n",
    "        ax1.plot(batch_sizes, latencies, 'o-')\n",
    "        ax1.set_title('Latency vs Batch Size')\n",
    "        ax1.set_xlabel('Batch Size')\n",
    "        ax1.set_ylabel('Latency (s)')\n",
    "        \n",
    "        # Throughput vs Batch Size\n",
    "        ax2.plot(batch_sizes, throughputs, 'o-')\n",
    "        ax2.set_title('Throughput vs Batch Size')\n",
    "        ax2.set_xlabel('Batch Size')\n",
    "        ax2.set_ylabel('Throughput (actions/s)')\n",
    "        \n",
    "        # Cost Analysis\n",
    "        ax3.bar(range(len(costs)), costs)\n",
    "        ax3.set_title('Cost per Configuration')\n",
    "        ax3.set_xlabel('Configuration Index')\n",
    "        ax3.set_ylabel('Cost ($)')\n",
    "        \n",
    "        # Memory Usage (if available)\n",
    "        if self.memory_usage:\n",
    "            ax4.plot(batch_sizes, self.memory_usage, 'o-')\n",
    "            ax4.set_title('Memory Usage vs Batch Size')\n",
    "            ax4.set_xlabel('Batch Size')\n",
    "            ax4.set_ylabel('Memory (MB)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "optimizer = PipelineOptimizer(env)\n",
    "\n",
    "param_grid = {\n",
    "    'batch_sizes': [1, 2, 4, 8, 16],\n",
    "    'models': ['gpt-3.5-turbo-instruct'],\n",
    "    'use_cache': [True, False],\n",
    "    'max_concurrent': [1, 2, 4]\n",
    "}\n",
    "\n",
    "results = optimizer.optimize_pipeline(param_grid)\n",
    "optimizer.plot_results(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SayCan_Robot_Pick_Place.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "saycan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
